\documentclass[acmsmall, nonacm, screen]{acmart}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{anyfontsize}

\newif\ifdraft\drafttrue

\definecolor{green}{HTML}{298a33}
\definecolor{orange}{HTML}{995d02}

\newcommand{\outline}[1]{
  \ifdraft
  {\color{red}{#1}}
  \fi
}

\newcommand\doverline[1]{%
  \setbox0=\hbox{$\overline{#1}$}%
  \ht0=\dimexpr\ht0-.30ex\relax% CHANGE .15 TO AFFECT SPACING
  \overline{\copy0}%
}

\newcommand{\ifThenElse}[3]{\textsf{\color{ACMDarkBlue}if}~#1~\textsf{\color{ACMDarkBlue}then}~#2~\textsf{\color{ACMDarkBlue}else}~#3}
\newcommand{\caseOf}[2]{\textsf{\color{ACMDarkBlue} case}~#1~\textsf{\color{ACMDarkBlue}of}~\{~#2~\}}
\newcommand{\letIn}[3]{\textsf{\color{ACMDarkBlue}let}~#1 = #2~\textsf{\color{ACMDarkBlue}in}~#3}
\newcommand{\shift}[2]{\textsf{\color{ACMDarkBlue}shift}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\callcc}[2]{\textsf{\color{ACMDarkBlue}call/cc}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\reset}[1]{\langle #1 \rangle}
\newcommand{\lambdaE}[2]{\lambda #1.\, #2}
\newcommand{\just}[1]{\textsf{Just}~#1}
\newcommand{\nothing}{\textsf{Nothing}}
\newcommand{\map}[3]{\textsf{map}^{\textsf{#1}}~#2~#3}
\newcommand{\unit}[2]{\textsf{unit}^{\textsf{#1}}~#2}
\newcommand{\join}[2]{\textsf{join}^{\textsf{#1}}~#2}
\newcommand{\cps}[1]{\mathcal{C}\llbracket #1 \rrbracket}
\newcommand{\cpsm}[1]{\mathcal{C}'\llbracket #1 \rrbracket}
\newcommand{\cpsmc}[1]{\mathcal{C}''\llbracket #1 \rrbracket}
\newcommand{\denote}[1]{\mathcal{E}\llbracket #1 \rrbracket}
\newcommand{\stringE}[1]{\textsf{\color{green} ``#1''}}
\newcommand{\quoteE}[1]{{\color{orange} \ulcorner #1 \urcorner}}
\newcommand{\unquoteE}[1]{{\color{black} \llparenthesis #1 \rrparenthesis }}

\lstset{ %
  backgroundcolor=\color{white},
  commentstyle=\color{ACMGreen},
  keywordstyle=\color{ACMDarkBlue},
  stringstyle=\color{ACMPurple},
  basicstyle=\ttfamily
}

\lstdefinestyle{hs}{
  language=Haskell
}

\lstdefinestyle{rkt}{
  language=lisp,
  deletekeywords={get},
  morekeywords={define},
  literate=*{(}{{\textcolor{gray}{(}}}{1}
    {)}{{\textcolor{gray}{)}}}{1}
}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\acmBooktitle{}

\begin{document}

\title{Delimited Continuations and Monads}
\subtitle{A Comparison of Programming Abstractions}
\titlenote{
  This report was compiled for part of the University of Pennsylvania's WPE-II Exam. The
  accompanying talk is available on the author's website.
}

\author{Harrison Goldstein}
\email{hgo@seas.upenn.edu}
\orcid{0000−0001−9631−1169}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia, PA}
  \country{USA}
}

\renewcommand{\shortauthors}{Goldstein}

\begin{abstract}
  In 1990, two programming abstractions were introduced independently: {\em delimited
  continuations} and {\em monads}. \citet{danvy1990abstracting} explored delimited continuations
  as a principled way of manipulating program contexts as first-class functions.
  \citet{wadler1990comprehending} popularized monads as a tool for simulating effects in a pure
  language. Though they seem to have little to do with one-another, delimited continuations and
  monads actually have a lot in common: they can solve many of the same programming problems, and
  their meta-theories are surprisingly compatible. This report re-contextualizes those early 90's
  papers and explores the commonalities between monads and delimited continuations.
\end{abstract}

\maketitle

\section{Introduction} \label{sec:introduction}
\outline{Talk briefly about compositional abstractions---what they are and why they matter for PL}
Programming languages research focuses heavily on abstractions. Abstractions enable programmers
to write code that is succinct and clear while hiding implementation details. Consider the
following programs that appear to be stateful:
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=rkt]
  (define (program)
    (let ([i (get)])
      (put (+ i 3))
      (let ([j (get)])
        (put (* j 7))
        (get))))

  (eval-state program 3) ; 42
  \end{lstlisting}
  \label{fig:racket-state}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=hs]
  program = do
    i <- get
    put (i + 3)
    j <- get
    put (j * 7)
    get

  evalState program 3 -- 42
  \end{lstlisting}
  \label{fig:hs-state}
\end{subfigure}
\Description[State Abstraction]{Code for manipulating state.}
\label{fig:stateful-comps}
\end{figure}
\noindent Perhaps surprisingly, both of these programs are ``purely'' functional---they do not
use references or other imperative features. The illusion of state is created by underlying
abstractions that allow the programmer to think in terms of changing state without sacrificing
the advantages of purity.

The first snippet, written in Racket, uses {\em delimited continuations} to implement a stateful
API. A continuation is a first-class representation of the "rest" of a computation. For example,
when evaluating the sub-expression ``$x < 3$'' in
\[ \ifThenElse{x < 3}{10}{20}, \]
the remaining {\em context} is
\[ \ifThenElse{\_}{10}{20}, \]
and the continuation, $k$, is
\[ k = \lambdaE{b}{\ifThenElse{b}{10}{20}}. \]
The continuation captures the context as a function. In {\em Abstracting Control},
\citet{danvy1990abstracting} present two operators for manipulating continuations: {\em shift}
(written ``$\shift{k}{e}$'') and {\em reset} (written ``$\reset{e}$''). Reset delimits the scope
of the current context---in the example above, we implicitly wrap the whole expression in a
reset. The expression ``$\shift{k}{e}$''captures the current context as a continuation, $k$, and
then executes $e$ in a fresh context. We can use shift and reset to extract the continuation from
above:
\begin{align*}
  & \reset{\ifThenElse{(\shift{k}{k~(x < 3)})}{10}{20}} \\
  \rightarrow\ & \letIn{k}{(\lambdaE{b}{\ifThenElse{b}{10}{20}})}{k~(x < 3)}
\end{align*}
Reset delimits the context, in this case the whole ``$\textsf{if}$'' expression, and shift
packages the continuation into $k$. In this example the shift body just applies $k$, so nothing
has changed---the result will be the same as the original expression. But the shift body can be
anything. In particular, if $k$ is not used at all
\[ \reset{\ifThenElse{(\shift{k}{x < 3})}{10}{20}} \]
the result is essentially an {\em exception}: the ``$\textsf{if}$'' is thrown away and the result
of the computation is just $x < 3$. Clever uses of delimited continuations like this can be used
to build up complex control flow including the state abstraction from our original example.

The second snippet simulates stateful computations using a {\em monad} in Haskell. Monads were
introduced to the programming languages literature by \citet{moggi1991notions}, but they were
popularized by \citeauthor{wadler1990comprehending} in {\em Comprehending
Monads}~\cite{wadler1990comprehending}. As a programming abstraction, monads generalize the
well-known idea of list comprehensions:
\[
  [ (x,\,y) \mid x \leftarrow [1,\, 2] ;\ y \leftarrow [3,\, 4] ]^{\textsf{List}}
\]
This expression results in the Cartesian product of lists ``$[1,\, 2]$'' and ``$[3,\, 4]$.''
Wadler observed that comprehension syntax can be to program with any monad (not just
$\textsf{List}$):
\[
  [ y \mid x \leftarrow \textsf{thisMightFail}~() ;\ y \leftarrow \textsf{thisMightFailToo}~{x} ]^{\textsf{Maybe}}
\]
This monad, usually called $\textsf{Maybe}$, implements a form of exceptions: if either of the
provided computations fail, the result of comprehension will be $\nothing$, otherwise the result
will be the value stored in $y$. There are dozens of monads that programmers use in practice,
each of which gives a different interpretation of comprehension syntax. One such monad,
$\textsf{State}$, can be used to implement our running example.

It is not immediately clear that manipulating continuations has anything to do with interpreting
comprehension syntax. One would be forgiven for assuming that these ideas are largely orthogonal.
But we have already seen that that delimited continuations and monads can be used to implement
some of the same design patterns. Could there be more overlap? Yes! It turns out that that are
many programming techniques, including state, exceptions, non-determinism, and more, that can be
implemented using either delimited continuations or monads. Furthermore, continuations actually
form a monad, and \citeauthor{wadler1990comprehending} shows that this fact can be exploited to
to recover results from \citeauthor{danvy1989functional}.

In this report I make three contributions, after a brief discussion of common notation
(\S~\ref{sec:notation}):
\begin{itemize}
  \item I summarize and re-contextualize two influential papers: {\em Abstracting Control} by
  \citeauthor{danvy1990abstracting} (\S~\ref{sec:danvy}) and {\em Comprehending Monads} by
  \citeauthor{wadler1990comprehending} (\S~\ref{sec:wadler}).\footnote{This is a requirement of
  the WPE-II exam, but I hope that reformulating these results will still be instructive.}
  \item I show the many design patterns that can be implemented equally well by both delimited
  continuations and monads, establishing a close relationship between the two language features
  (\S~\ref{sec:patterns}).
  \item I explore the {\em continuation monad} and show that by performing a monad-agnostic
  transformation given by \citeauthor{wadler1990comprehending} we can recover the ``extended
  continuation passing style'' transformation presented by \citeauthor{danvy1989functional}
  (\S~\ref{sec:contmonad}). This is a small extension of Wadler's original result.
\end{itemize}
I conclude with some remarks on the broader context in which these abstractions exist
(\S~\ref{sec:conclusion}).

\section{Notes on Notation} \label{sec:notation}
The two source papers used different notations and conventions---this report aims to keep as much
common notation as possible. For all of the presentation, assume that programs are written in a
lambda calculus given by the grammar
\begin{align*}
  e \Coloneqq &\ x \mid \lambdaE{x}{e} \mid e_1~e_2 \\
             | &\ \textsf{\color{ACMDarkBlue} true} \mid \textsf{\color{ACMDarkBlue} false} \mid \ifThenElse{e_1}{e_2}{e_3} \\
             | &\ (e_1,\, e_2) \mid \textsf{\color{ACMDarkBlue} fst}~e \mid \textsf{\color{ACMDarkBlue} snd}~e.
\end{align*}
We are almost exclusively concerned with the dynamic semantics of these programs, so I will not
formalize a type system. Occasionally I will ignore either Booleans or pairs for brevity.

Those already familiar with delimited continuations might be used to a different notation than
the one I use here. Some papers use ``$\xi k.\, e$'' where I will use ``$\shift{k}{e}$.'' Also,
``$\textsf{\color{ACMDarkBlue} reset}~e$'' is used instead of my ``$\reset{e}$.''

Those familiar with monads might expect to see ``bind'', written \textsf{>>=}. The two
presentations are essentially equivalent. Also, Haskell users will note that Wadler's monad
comprehensions are very similar to ``do--notation''---the conversion between the two syntaxes is
left as an exercise to the reader.

\section{{\em Abstracting Control}} \label{sec:danvy}

\citeauthor{danvy1990abstracting} begin their discussion of delimited continuations by exploring
the standard {\em continuation-passing style} or CPS translation. Here is the translation,
$\cps{\cdot}$, for our basic lambda calculus with Booleans and if-statements:
\begin{align*}
  \cps{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cps{\lambdaE{x}{e}} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{\cps{e}})} \\
  \cps{e_1~e_2} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \\
  \cps{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cps{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{b}{\ifThenElse{b}{\cps{e_2}~\kappa}{\cps{e_3}~\kappa}})}
\end{align*}
Note how evaluation order is made explicit---this CPS translation is clearly call-by-value, since
in the rule for application,
\[ \cps{e_1~e_2} = \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \]
both arguments are evaluated before the application is finally computed. The CPS translation is a
valuable tool for compilers and interpreters, where more explicit control flow often simplifies
other considerations.

Prior to this work, researchers had already considered language-level abstractions for working
with continuations. The most famous, known as ``call with current continuation,'' or simply
call/cc can be added as an operator in our source language and implemented as an extension to the
CPS translation:
\[ \cps{\callcc{k}{e}} = (\lambdaE{\kappa}{\cps{e}~\kappa})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa~x}}] \]
where ``$e[x \mapsto v]$'' means $e$ with $v$ substituted for free instances of $x$. The call/cc
operator is extremely powerful, allowing the programmer to arbitrarily pause and resume
evaluation as they see fit. For example,
\[ \callcc{k}{1} \]
throws an exception with the value $1$ and
\[ \callcc{k}{(\textsf{\color{ACMDarkBlue} print}~\stringE{foo};\ k~10)} \]
pauses the computation, prints ``foo'', and then resumes computation with the value 10.
Unfortunately, many have said that call/cc is {\em too} powerful. Consider the popular Yin-Yang
Puzzle:
\begin{align*}
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yin}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{@};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yang}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{*};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{yin}~\textsf{yang}
\end{align*}
What does this confusing mess of continuations do? Apparently it counts indefinitely in a unary
representation, printing
\[ \stringE{@*@**@***@****@*****...}, \]
but it is extremely difficult to understand exactly why that is.

\citet{felleisen1988theory} started to reign this power in, introducing first-class ``prompts'' and
a way to delimit continuation scopes, but scopes were dynamic and the overall approach did not admit
a straightforward translation into a standard lambda calculus. \citeauthor{danvy1990abstracting}
built on this approach and introduced statically delimited continuations.

\subsection{Delimited Continuations}

\citeauthor{danvy1990abstracting} introduce the ``shift'' and ``reset'' operations mentioned in
Section \ref{sec:introduction}, which provide a more usable alternative to call/cc.
\[
  e \Coloneqq \cdots \mid \shift{k}{e} \mid \reset{e}
\]
These operators can also be interpreted via a modified CPS translation into what Danvy and
Filinski call
{\em extended continuation-passing style} (ECPS):
\begin{align*}
  \cps{\shift{k}{e}} &= (\lambdaE{\kappa}{\cps{e}~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \cps{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cps{e}~\textsf{id})}
\end{align*}
Shift and reset provide a more intuitive way of working with continuations. Reset clearly
delimits the bounds of the current context, and shift gives programmatic access to that context.
In this example
\begin{align*}
& 1 + \reset{10 + \shift{k}{k~(k~100)}} \Rightarrow \\
& 1 + (10 + (10 + 100)) \Rightarrow \\
& 121,
\end{align*}
the continuation ``$k = \lambdaE{x}{10 + x}$'' is applied twice to $100$ before finishing the
evaluation by adding $1$.

In addition to the CPS translation, \citeauthor{danvy1990abstracting} give a denotational semantics.
Assuming $\textsf{Ans}$ is a suitable domain of final answers, we define
\begin{center}
  \begin{tabular}{llr}
    $\rho \in \textsf{Env}$ & $=$ & $\textsf{Var} \rightharpoonup \textsf{Val}$ \\
    $\gamma \in \textsf{MCont}$ & $=$ & $\textsf{Val} \to \textsf{Ans}$ \\
    $\kappa \in \textsf{Cont}$ & $=$ & $\textsf{Val} \to \textsf{MCont} \to \textsf{Ans}$ \\
    $\mathcal{E}$ & $:$ & $\textsf{Exp} \to \textsf{Env} \to \textsf{Cont} \to \textsf{MCont} \to \textsf{Ans}$.
  \end{tabular}
\end{center}
The denotational semantics is given by the equations
\begin{align*}
  \denote{x}~\rho~\kappa~\gamma &= \kappa~(\rho[x])~\gamma \\
  \denote{\lambdaE{x}{e}}~\rho~\kappa~\gamma &= \kappa~(\lambdaE{v}{\denote{e}~(\rho[x \mapsto v])})~\gamma \\
  \denote{e_1~e_2}~\rho~\kappa~\gamma &=
    \denote{e_1}~\rho~(\lambdaE{f}{\denote{e_2}~\rho~(\lambdaE{x}{f~x~\kappa})})~\gamma \\
  \denote{\textsf{\color{ACMDarkBlue}true}}~\rho~\kappa~\gamma &= \kappa~\textsf{\color{ACMDarkBlue} true}~\gamma \\
  \denote{\ifThenElse{e_1}{e_2}{e_3}}~\rho~\kappa~\gamma &= 
    \denote{e_1}~\rho~(\lambdaE{b}{\ifThenElse{b}{\denote{e_2}~\rho~\kappa}{\denote{e_3}~\rho~\kappa}})~\gamma \\
  \denote{\shift{k}{e}}~\rho~\kappa~\gamma &=
    \denote{e}~(\rho[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\lambdaE{\gamma'}{\kappa~x~(\lambdaE{w}{\kappa'~w~\gamma'})}}}])~(\lambdaE{x}{\lambdaE{\gamma''}{\gamma''~x}})~\gamma \\
  \denote{\reset{e}}~\rho~\kappa~\gamma &= \denote{e}~\rho~(\lambdaE{x}{\lambdaE{\gamma'}{\gamma'~x}})~(\lambdaE{x}{\kappa~x~\gamma})
\end{align*}
where constructions on the right-hand side are understood as syntax of a meta-language for
defining computable functions, rather than as concrete syntax of a programming language. Note that
some $\gamma$ arguments are $\eta$-reduced away to simplify presentation---in fact, they can be
entirely elided for all rules other than the ones for shift and reset.

\subsection{Metacircular Interpreters}

The term {\em metacircular interpreter} was popularized by \citet{reynolds1972definitional}, and
characterizes the practice of defining a language's semantics based on an interpreter written in
a similar language. Danvy and Filinski use this approach to build a more efficient ECPS
translation.

The ECPS translation can be significantly optimized with the help of a very simple meta-language.
In the following translation, $\quoteE{\text{quoted}}$ text represents the translation output
while black text represents the syntax of the interpreter. We use $\unquoteE{\text{unquote
brackets}}$ to inject the result of meta-computations into quoted output.
\begin{align*}
  \cpsm{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cpsm{\lambdaE{x}{e}} &=
    \lambdaE{\kappa}{\kappa~\quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{k}{\unquoteE{\cpsm{e}~(\lambdaE{a}{\quoteE{k~\unquoteE{a}}})}}}}} \\
  \cpsm{e_1~e_2} &= \lambdaE{\kappa}{\cpsm{e_1}~(\lambdaE{f}{\cpsm{e_2}~(\lambdaE{x}{\quoteE{\unquoteE{f}~\unquoteE{x}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}})})} \\
  \cpsm{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cpsm{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cpsm{e_1}~(\quoteE{\lambdaE{b}{\ifThenElse{b}{\unquoteE{\cpsm{e_2}~\kappa}}{\unquoteE{\cpsm{e_3}~\kappa}}}})} \\
  \cpsm{\shift{k}{e}} &= (\lambdaE{\kappa}{\cpsm{e}~\textsf{id}})[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}] \\
  \cpsm{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cpsm{e}~\textsf{id})}
\end{align*}
The final ECPS result is given by ``$\cpsm{e}~\textsf{id}$''. This optimized translation avoids
inserting unnecessary redexes, and produces a much simpler ECPS result.

It turns out that we can do even better if we include delimited continuation operators in our
meta-language. The final ECPS translation, 
\begin{align*}
  \cpsmc{x} &= x \\
  \cpsmc{\lambdaE{x}{e}} &= \quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{\kappa}{\unquoteE{\reset{\quoteE{\kappa~\unquoteE{\cpsmc{e}}}}}}}} \\
  \cpsmc{e_1~e_2} &= \shift{\kappa}{\quoteE{\unquoteE{\cpsmc{e_1}}~\unquoteE{\cpsmc{e_2}}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}} \\
  \cpsmc{\textsf{\color{ACMDarkBlue}true}} &= \textsf{\color{ACMDarkBlue}true} \\
  \cpsmc{\ifThenElse{e_1}{e_2}{e_3}} &= \shift{\kappa}{\quoteE{\ifThenElse{\unquoteE{\cpsmc{e_1}}}{\unquoteE{\reset{\kappa~\cpsmc{e_2}}}}{\unquoteE{\reset{\kappa~\cpsmc{e_3}}}}}} \\
  \cpsmc{\shift{k}{e}} &= \shift{\kappa}{\reset{\cpsmc{e}}[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}]} \\
  \cpsmc{\reset{e}} &= \reset{\cpsmc{e}}
\end{align*}
also avoids unnecessary $\eta$-expansion, resulting in an extremely compact final representation.

\subsection{Use Case: Nondeterministic Programming}

Delimited control operators can be used to solve complex programming problems. Danvy and Filinski
choose to highlight one application in particular: nondeterministic programming. The nondeterministic
operators \textsf{fail}, \textsf{flip}, and \textsf{choice} are defined first:
\begin{align*}
\textsf{fail}~() &= \shift{k}{\stringE{failure}} \\
\textsf{flip}~() &= \shift{k}{(k~\textsf{\color{ACMDarkBlue}true};\ k~\textsf{\color{ACMDarkBlue}false};\ \textsf{fail}~())} \\
\textsf{choice}~n &= \ifThenElse{n < 1}{\textsf{fail~()}}{\ifThenElse{\textsf{flip}~()}{\textsf{choice}~(n - 1)}{n}}
\end{align*}
The \textsf{fail} operation just acts as an exception, throwing away the continuation and
returning the string $\stringE{failure}$. The main source of nondeterminism is \textsf{flip}---it
actually calls its continuation on {\em both} \textsf{\color{ACMDarkBlue}true} and
\textsf{\color{ACMDarkBlue}false}, but the caller is expected to use \textsf{flip} as if it
nondeterministically chooses between the two options (in model of nondeterminism, those two
points of view coincide). Finally, \textsf{choice} builds on \textsf{flip} to simulate a
nondeterministic choice of integers less than a given value.

The provided operators can be used to implement
\begin{align*}
\textsf{triple}~n~s =&  \\
& \textsf{\color{ACMDarkBlue}let}~i = \textsf{choice}~n~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~j = \textsf{choice}~(i - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~k = \textsf{choice}~(j - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \ifThenElse{i + j + k = s}{(i,\, j,\, k)}{\textsf{fail}~()},
\end{align*}
which finds all triples of distinct positive integers that sum to a given integer $s$. Evaluating
the expression ``$\reset{\textsf{\color{ACMDarkBlue}print}~(\textsf{triple}~9~15)}$'' prints all
triples of integers up to $9$ that sum to $15$.

The \textsf{triple} example is mostly a toy, but these primitives can be practically useful. In
the paper Danvy and Filinski show an evaluator for {\em nondeterministic finite automata} (NFAs),
which is straightforward to implement thanks to operators like \textsf{flip} and \textsf{fail}.
The implementation is not especially instructive, so I will not repeat it here.

\subsection{Generalizing to More Contexts}

The final contribution of {\em Abstracting Control} is an idea for a family of shift and reset
operators that operate on an indexed family of program contexts. They introduce operators
\begin{center}
  $\textsf{\color{ACMDarkBlue}shift}_i~k~\textsf{\color{ACMDarkBlue}in}~e$ \hspace{5mm}
  and \hspace{5mm} $\reset{e}_i$
\end{center}
which access and delimit the $i$th enclosing context. These operators can again be given meaning
via the ECPS translation, but note that a program that uses indices up to $n$ will need to be
translated $n + 1$ times. The translations are given by induction (for simplicity, I chose the
non-metacircular version):
\begin{align*}
  \cps{\textsf{\color{ACMDarkBlue}shift}_0~k~\textsf{\color{ACMDarkBlue}in}~e} &= e[k \mapsto \textsf{id}] \\
  \cps{\textsf{\color{ACMDarkBlue}shift}_{n + 1}~k~\textsf{\color{ACMDarkBlue}in}~e} &= 
    \lambdaE{\kappa}{\textsf{\color{ACMDarkBlue}shift}_{n}~k'~\textsf{\color{ACMDarkBlue}in}~(\cps{e}~\textsf{id})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\reset{k'~(\kappa~x)}_n}}]} \\
  \cps{\reset{e}_0} &= e \\
  \cps{\reset{e}_{n + 1}} &= \lambdaE{\kappa}{\kappa~\reset{\cps{e}~\textsf{id}}_n} \\
\end{align*}

This indexed family of operators can be used to interleave various effects. For example, the
operator
\[ \textsf{emit}~n =
\cps{\textsf{\color{ACMDarkBlue}shift}_2~k~\textsf{\color{ACMDarkBlue}in}~\textsf{Cons}~n~(k~\textsf{Nil})}
\]
can be used in tandem with \textsf{choice} (defined using
$\textsf{\color{ACMDarkBlue}shift}_1$) to make \textsf{triple} produce a list of triples, rather
than passing triples to the caller one at a time.

Danvy and Filinski discuss some of the meta-theory behind this approach, but I will not discuss
that here.

\section{{\em Comprehending Monads}} \label{sec:wadler}

Monads were originally introduced to the programming languages literature by
\citet{moggi1991notions}, but {\em Comprehending Monads} by \citet{wadler1990comprehending} made
them the popular programming abstraction that they are today. A monad is a common structure in
category theory. There, a monad is defined as a \underline{functor} with certain associated
\underline{\smash{operations}} that obey a set of \underline{laws}.

For our purposes, a functor is a type constructor (e.g. \textsf{List}, \textsf{Maybe}) with an
operation \textsf{map} that applies a function ``under'' the type constructor. For example,
\[ \map{List}{f}{\overline{x}} \]
applies $f$ to all elements of $\overline{x}$ and 
\[ \map{Maybe}{f}{\overline{x}} \]
applies $f$ to the value contained in $\overline{x}$, if one exists. (I adopt Wadler's convention
of writing monadic values---values wrapped in the monad's type constructor---with a bar over the
variable name.) Map must obey two laws:
\begin{align*}
  \textsf{map}~\textsf{id} &= \textsf{id} \\
  \textsf{map}~(g \circ f) &= \textsf{map}~g \circ \textsf{map}~f
\end{align*}

In order for a functor to be a monad, it needs two more operations: \textsf{unit} and \textsf{join}.
Applying \textsf{unit} to a value injects that value into the monad. In the case of \textsf{List},
\[ \unit{List}{x} = \textsf{singleton}~x = [x], \]
which is the simplest way to inject a value into a list. Likewise,
\[ \unit{Maybe}{x} = \textsf{Just}~x. \]

The \textsf{join} operation works on ``doubled up'' instances of the monad. It takes a doubled
value like
\begin{center}
  ``$[[1, 2], [3, 4]]$'' or ``$\textsf{Just}~(\textsf{Just}~5)$''
\end{center}
and flattens it down to a single application of the monad like
\begin{center}
  ``$[1, 2, 3, 4]$'' or ``$\textsf{Just}~5$''.
\end{center}
To be precise,
\begin{center}
  \begin{tabular}{lll}
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
\end{center}

As mentioned, these operations must obey certain laws for the monad to behave properly:
\begin{align}
  \textsf{map}~f \circ \textsf{unit} &= \textsf{unit} \circ f \\
  \textsf{map}~f \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~(\textsf{map}~f) \\
  \textsf{join} \circ \textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{map}~\textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~\textsf{join}
\end{align}
Intuitively, laws (1) and (2) show that \textsf{unit} and \textsf{join} are ``well-behaved'' with
respect to \textsf{map},\footnote{Wadler notes that (1) and (2) hold automatically if the
operations $\textsf{map}$, $\textsf{unit}$, and $\textsf{join}$ are implemented as {\em
polymorphic functions}, but those details are out of scope for this
report~\cite{wadler1989theorems}.} and laws (3-5) require that \textsf{unit} and \textsf{join}
are the {\em simplest} ways to do their respective tasks. For example, suppose we tried to implement
\[ \unit{List}{x} = [x,\, x,\, x,\, x,\, x]. \]
Technically this does inject a value into a list, but it is not the simplest way to do so and
\[
  (\textsf{join}^{\textsf{List}} \circ \textsf{unit}^{\textsf{List}})~[3] = [3,\, 3,\, 3,\, 3,\, 3] \neq \textsf{id}~[3]. \\
\]
Likewise, the laws require that \textsf{join} flattens a stack of monads in a simple, predictable
way.

All of this background on monads and their operations is important, but in practice using these
operations directly does not make for particularly elegant code. In the next section, I explore
Wadler's use of {\em comprehensions} to make programming with monads much more intuitive.

\subsection{Comprehensions}

List comprehensions are based on set-builder notation (e.g. $\{n + 1 \mid n \in \mathbb{N}\}$)
and were first introduced to programming by the SETL language in the
1960's~\cite{schwartz2012programming}. Since then, list comprehensions have become ubiquitous,
appearing in many modern languages including Racket, Python, and even C++ (with some abuse of
operator overloading). As shown in Section \ref{sec:introduction},
\[ [(x, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{List}} \]
represents the Cartesian product of $\overline{x}$ and $\overline{y}$ in list-comprehension notation
\[ \{(x, y) \mid x \in X \wedge y \in Y\} \]
represents the Cartesian product of $X$ and $Y$ in standard mathematical notation.

Wadler starts off by adding a formal syntax for list comprehensions to a standard lambda calculus:
\begin{align*}
 e &\Coloneqq \cdots \mid [e \mid q] \\
 q &\Coloneqq \Lambda \mid x \leftarrow e \mid (q_1;\ q_2)
\end{align*}
The symbol $q$ stands for {\em qualifiers}---qualifiers can be empty (denoted $\Lambda$), a {\em
generator} for a variable $x$, or a composition of two qualifiers. This list comprehension syntax
is defined by the following rules:
\begin{align*}
  [e \mid \Lambda] &= \textsf{singleton}~e \\
  [e_1 \mid x \leftarrow e_2] &= \textsf{map}~(\lambdaE{x}{e_1})~e_2 \\
  [e \mid (q_1;\ q_2)] &= \textsf{flatten}~[[e \mid q_2] \mid q_1] \\
\end{align*}
Wadler noticed that lists are not the only structure for which these operations make
sense---recall that \textsf{singleton} is $\textsf{unit}^{\textsf{List}}$ and \textsf{flatten} is
$\textsf{join}^{\textsf{List}}$. This means that comprehensions actually work for any monad, with
their behavior defined by:
\begin{align*}
  [e \mid \Lambda]^{\textsf{M}} &= \unit{M}{e} \\
  [e_1 \mid x \leftarrow e_2]^{\textsf{M}} &= \map{M}{(\lambdaE{x}{e_1})}{e_2} \\
  [e \mid (q_1;\ q_2)]^{\textsf{M}} &= \join{M}{[[e \mid q_2]^{\textsf{M}} \mid q_1]^{\textsf{M}}} \\
\end{align*}
Rewriting the \textsf{Maybe} example from Section \ref{sec:introduction} using these rules (and
rewriting a bit) results in
\begin{align*}
  &[ y \mid x \leftarrow \textsf{thisMightFail}~() ;\ y \leftarrow \textsf{thisMightFailToo}~{x} ]^{\textsf{Maybe}} = \\
  &\join{}{(\map{}{\textsf{thisMightFailToo}}{(\textsf{thisMightFail}~())})}
\end{align*}
which has the semantics we expect: if either computation fails the final result is \nothing\,
otherwise the result is a single \textsf{Just} wrapped around the final result.

\subsection{Examples of Monads} \label{sec:monad-examples}

In this section I list of the most common and useful monads that Wadler points out.

\subsubsection{List and Maybe}
The first two monads have been running examples in this section, so I will not spend much time on
them here. Here is the full definition of the list monad
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type List}$~$\alpha$ & $=$ & $\textsf{Nil} \mid \textsf{Cons}~\alpha~(\textsf{List}~\alpha)$ \\
    $\map{List}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nil} \to \textsf{Nil};\ \textsf{Cons}~y~\overline{y} \to \textsf{Cons}~(f~y)~(\map{List}{f}{\overline{y}})}$ \\
    $\unit{List}{x}$ & $=$ & $\textsf{singleton}~x$ \\
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$
  \end{tabular}
\end{center}
and the maybe monad
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Maybe}$~$\alpha$ & $=$ & $\textsf{Nothing} \mid \textsf{Just}~\alpha$ \\
    $\map{Maybe}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nothing} \to \textsf{Nothing};\ \textsf{Just}~y \to \textsf{Just}~(f~y)}$ \\
    $\unit{Maybe}{x}$ & $=$ & $\textsf{Just}~x$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
\end{center}
The type definitions are informal and based loosely on Haskell's algebraic data type syntax.

\subsubsection{Identity and Strictness}
The simplest possible monad is the identity functor.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Id}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Id}{f}{\overline{x}}$ & $=$ & $f~\overline{x}$ \\
    $\unit{Id}{x}$ & $=$ & $x$ \\
    $\join{Id}{\doverline{x}}$ & $=$ & $\doverline{x}$
  \end{tabular}
\end{center}
This monad is almost not worth mentioning, but it does have one cute use-case: comprehension
syntax for the identity monad subsumes \textsf{\color{ACMDarkBlue} let} binding. Rather than
write
\begin{center}
  $\letIn{x}{e_1}{e_2}$ \hspace{5mm} we can write \hspace{5mm} $[e_2 \mid x \leftarrow e_1]^{\textsf{Id}}$.
\end{center}
There is no real reason to prefer this syntax, so I will continue to use
$\textsf{\color{ACMDarkBlue} let}$, but it is a fun observation.

A slight variation on the identity monad, the {\em strictness} monad, does actually have some
interesting uses.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Str}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Str}{f}{\overline{x}}$ & $=$ & $\ifThenElse{\overline{x} \neq \bot}{f~\overline{x}}{\bot}$ \\
    $\unit{Str}{x}$ & $=$ & $x$ \\
    $\join{Str}{\doverline{x}}$ & $=$ & $\doverline{x}$
  \end{tabular}
\end{center}
The only difference here is the implementation of $\textsf{map}$---the strictness monad enforces
call-by-value application of the function $f$. In a call-by-name language with divergence, the
strictness monad provides a convenient syntax for controlling evaluation order.

\subsubsection{State}
Another monad we have already seen is the \textsf{State} monad. The state monad makes it possible
to write code that {\em looks} stateful, even though under the hood there is a state parameter
being passed around. Assuming some fixed state type, $\sigma$, the state monad is defined as:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type State}$~$\alpha$ & $=$ & $\sigma \to (\alpha,\, \sigma)$ \\
    $\map{State}{f}{\overline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(x,\, s')}{\overline{x}~s}{(f~x,\, s')}}$ \\
    $\unit{State}{x}$ & $=$ & $\lambdaE{s}{(x,\, s)}$ \\
    $\join{State}{\doverline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(\overline{x},\, s')}{\doverline{x}~s}{\overline{x}~s'}}$
  \end{tabular}
\end{center}
As with many monads, the real magic comes with some auxiliary operations. In this case, defining
\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\lambdaE{s}{(s,\, s)}$ \\
    $\textsf{put}~s'$ & $=$ & $\lambdaE{s}{(x,\, s')}$
  \end{tabular}
\end{center}
gives us everything we need to implement the example from the introduction:
\[ [k \mid i \leftarrow \textsf{get};\ \_ \leftarrow \textsf{put}~(i + 3);\ j \leftarrow \textsf{get};\ \_ \leftarrow \textsf{put}~(j * 7);\ k \leftarrow \textsf{get}]^{\textsf{State}} \]
This is a good time to mention that modern languages with monads use a variation of Wadler's
comprehension syntax called ``do-notation.'' The \textsf{do} from the Haskell example in Section
\ref{sec:introduction},
\[ \textsf{\color{ACMDarkBlue} do}\ \{\ i \leftarrow \textsf{get};\ \textsf{put}~(i + 3);\ j \leftarrow \textsf{get};\ \textsf{put}~(j * 7);\ \textsf{get}\ \}, \]
has two ergonomic optimizations. First, the whole expression evaluates to the final qualifier in
the list, which often saves a few characters, and second bare expressions are allowed as
qualifiers. These facts together mean that do-notation is a bit easier to use than Wadler's
syntax, even if it is morally quite similar. I discuss a bit more about the state of the art
in Section \ref{sec:conclusion}.

\subsubsection{Reader}
(Wadler calls this {\em State Reader}, but modern languages almost exclusively drop the word
``state'' and just call this monad ``reader.'') The \textsf{Reader} monad is a simplification of
the \textsf{State} monad, and arose when Wadler realized some parts of a \textsf{State}-monad
computation do not actually change the state (and in fact they {\em should} not).
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Reader}$~$\alpha$ & $=$ & $\rho \to \alpha$ \\
    $\map{Reader}{f}{\overline{x}}$ & $=$ & $\lambdaE{r}{f~(\overline{x}~r)}$ \\
    $\unit{Reader}{x}$ & $=$ & $\lambdaE{r}{x}$ \\
    $\join{Reader}{\doverline{x}}$ & $=$ & $\lambdaE{r}{(\doverline{x}~r)~r}$
  \end{tabular}
\end{center}
Despite its relative simplicity, the \textsf{Reader} monad is quite powerful. After defining the
auxiliary operation,
\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\lambdaE{r}{r}$,
  \end{tabular}
\end{center}
\textsf{Reader} can be used to keep track of configuration values, environment information, and
any other static value that would otherwise be cumbersome to pass around. Readers are also safer
and more flexible than global variables or constants, since they can be set midway through the
computation and are guaranteed to not change later on.

\subsubsection{Nondeterminism (Set)}
The nondeterminism monad (also known as the set monad) enables direct-style programming of
nondeterministic algorithms.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type ND}$~$\alpha$ & $=$ & $2^\alpha$ \\
    $\map{ND}{f}{\overline{x}}$ & $=$ & $\{f~x \mid x \in \overline{x}\}$ \\
    $\unit{ND}{x}$ & $=$ & $\{x\}$ \\
    $\join{ND}{\doverline{x}}$ & $=$ & $\bigcup \doverline{x}$
  \end{tabular}
\end{center}
A value in \textsf{ND} is a set of possible outcomes. Together with an auxiliary operator for
nondeterministic choice
\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\varnothing$ \\
    $\overline{x} \sqcup \overline{y}$ & $=$ & $\overline{x} \cup \overline{y}$,
  \end{tabular}
\end{center}
this monad can be extremely useful.

\subsubsection{Parser}
Finally, here is a more complicated monad: \textsf{Parser}. Parsers can be seen as a combination
of two monads: \textsf{List} and \textsf{State}.\footnote{Technically \textsf{Parser} can be
formed using a {\em monad transformer}, but those were developed later on~\cite{liang1995monad}.}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Parser}$~$\alpha$ & $=$ & $\textsf{String} \to \textsf{List}~(\alpha,\, \textsf{String})$ \\
    $\map{Parser}{f}{\overline{x}}$ & $=$ & $\lambdaE{i}{[(f~x,\, i') \mid (x,\, i') \leftarrow \overline{x}~i]^{\textsf{List}}}$ \\
    $\unit{Parser}{x}$ & $=$ & $\lambdaE{i}{[(x, i)]^{\textsf{List}}}$ \\
    $\join{Parser}{\doverline{x}}$ & $=$ & $\lambdaE{i}{[(x,\, i'') \mid (\overline{x},\, i') \leftarrow \doverline{x}~i;\ (x,\ i'') \leftarrow \overline{x}~i']^{\textsf{List}}}$
  \end{tabular}
\end{center}
Intuitively, the \textsf{String} argument is the parser input, and the result is a list of
potential parses. Each potential parse is made up of a result of type $\alpha$ and the
\textsf{String} that remains after parsing. There are a number of auxiliary operations that are
useful for \textsf{Parser} (often called {\em parser combinators}), including ``\textsf{satisfy}''
which only parses a character that matches a given predicate and ``\textsf{many}'' which applies a
parser multiple times. An amazing amount of research has been done on the \textsf{Parser} monad
and parser combinators~\cite{hutton1996monadic, leijen2001parsec}, and it is still an active
area~\cite{willis2020staged}.

\subsection{Translation}

Monads can be used to replicate a variety of effects in a pure way; it would be nice if there was
a way to translate an impure program into the equivalent monadic program. Wadler gives two such
translations, one for call-by-value and another for call-by-name. The result of both translations
is a pure program in the non-strict lambda calculus with comprehensions that we have been using for
examples.

Suppose our source language is our lambda running calculus with products and projections. The
call-by-value translation ``$e^*$'' lifts a program of type
\begin{center}
$\alpha \to \beta$ \hspace{5mm} to one of type \hspace{5mm} $\alpha \to \textsf{M}~\beta$
\end{center}
for some monad \textsf{M}. Intuitively, the translated function takes a value of type $\alpha$ and
returns a {\em computation} of type $\beta$. The monad captures the effects of that computation.
Here is the translation in full:
\begin{align*}
  x^* &= [x]^\textsf{M} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{M} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{M} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{M}
\end{align*}
Each of these rules has a straightforward computational meaning. For example
\[ (e_1~e_2)^* = [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \]
says that in order to evaluate a (potentially effectful) application, evaluate $e_1$ to $f$, then
evaluate $e_2$ to $x$, and then evaluate the application ``$f~x$'', all the while keeping track
of any side-effects that are produced.

If our source language was actually effectful, for example with built-in effects
``$\textsf{\color{ACMDarkBlue}get}$'' and ``$\textsf{\color{ACMDarkBlue}put}$'', we could choose
$\textsf{M} = \textsf{State}$ and add translations
\begin{align*}
  \textsf{\color{ACMDarkBlue}get}^* &= [x \mid x \leftarrow \textsf{get}]^\textsf{State} \\
  (\textsf{\color{ACMDarkBlue}put}~e)^* &= [u \mid x \leftarrow e^*;\ u \leftarrow \textsf{put}~x]^\textsf{State}
\end{align*}
where \textsf{get} and \textsf{put} are defined as in Section \ref{sec:monad-examples}. If we
were to prove once and for all that the monadic version has the same semantics as the impure
version, further analysis could be done on the pure language instead. In addition, this kind of
translation might streamline the implementation of a compiler or interpreter.

Wadler also gives a call-by-name version of the translation, which lifts a program of type
\begin{center}
$\alpha \to \beta$ \hspace{5mm} to one of type \hspace{5mm} $\textsf{M}~\alpha \to \textsf{M}~\beta$.
\end{center}
In call-by-name, the function arguments are computations as well. The translation is written
$e^\dagger$:
\begin{align*}
  x^\dagger &= x \\
  (\lambdaE{x}{e})^\dagger &= [\lambdaE{x}{e^\dagger}]^\textsf{M} \\
  (e_1~e_2)^\dagger &= [y \mid f \leftarrow e_1^\dagger;\ y \leftarrow f~e_2^\dagger]^\textsf{M} \\
  (e_1,\, e_2)^\dagger &= [(e_1^\dagger,\, e_2^\dagger)]^\textsf{M} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^\dagger &= [y \mid x \leftarrow e^\dagger;\ y \leftarrow \textsf{\color{ACMDarkBlue}fst}~x]^\textsf{M}
\end{align*}
If the source language were call-by-name, this translation might be similarly useful to the first.

\section{Common Design Patterns} \label{sec:patterns}
The zoo of monads presented by Wadler are a nice basis by which to evaluate other compositional
abstractions. As one might hope, delimited continuations are also able to represent many of the
monads Wadler mentions---a blog post by \citet{xia_2019} from 2019 describes these constructions
incredibly well, and I rely heavily on that post for this section.

Each of these examples will simply be a set of function definitions representing (1) the
primitive operations that are core to the abstraction and (2) a \textsf{run} function that delimits
the scope of the computation.

\subsubsection{Maybe}

\begin{center}
  \begin{tabular}{lll}
    $\textsf{abort}$ & $=$ & $\shift{k}{\textsf{Nothing}}$ \\
    $\textsf{run}^{\textsf{Maybe}}~c$ & $=$ & $\reset{\textsf{Just}~c}$
  \end{tabular}
\end{center}

\subsubsection{Identity}

\begin{center}
  \begin{tabular}{lll}
    $\textsf{noop}~x$ & $=$ & $\shift{k}{k~x}$ \\
    $\textsf{run}^{\textsf{Id}}~c$ & $=$ & $\reset{c}$
  \end{tabular}
\end{center}

\subsubsection{State}

\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\shift{k}{\lambdaE{s}{k~s~s}}$ \\
    $\textsf{put}~s'$ & $=$ & $\shift{k}{\lambdaE{s}{k~()~s'}}$ \\
    $\textsf{run}^{\textsf{State}}~c~s$ & $=$ & $\reset{(\lambdaE{v}{\lambdaE{s}{v}})~c}~i$
  \end{tabular}
\end{center}

\subsubsection{Reader}

\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\textsf{get}$ \\
    $\textsf{run}^{\textsf{Reader}}$ & $=$ & $\textsf{run}^{\textsf{State}}$
  \end{tabular}
\end{center}

\subsubsection{Nondeterminism}

\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\shift{k}{k~\varnothing}$ \\
    $x \sqcup y$ & $=$ & $\shift{k}{k~x \cup k~y}$ \\
    $\textsf{run}^{\textsf{ND}}~c$ & $=$ & $\reset{(\lambdaE{v}{\{v\}})~c}$
  \end{tabular}
\end{center}

\subsection{What about the rest?}
\outline{list, strict, parser}

\section{The Continuation Monad} \label{sec:contmonad}
So far we have seen a number of examples of monads, but \citeauthor{wadler1990comprehending}
actually discusses one more: the {\em continuation monad}. Given a result type, $\rho$, the
continuation monad has operations:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Cont}$~$\alpha$ & $=$ & $(\alpha \to \rho) \to \rho$ \\
    $\map{Cont}{f}{\overline{x}}$ & $=$ & $\lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\kappa~(f~x)})}$ \\
    $\unit{Cont}{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~x}$ \\
    $\join{Cont}{\doverline{x}}$ & $=$ &
      $\lambdaE{\kappa}{\doverline{x}~(\lambdaE{\overline{x}}{\overline{x}~\kappa})}$
  \end{tabular}
\end{center}
Programs written using the continuation monad are automatically in a continuation-passing style:
sequencing
\[ [(x, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{Cont}} \]
expands to
\[ \lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\overline{y}~(\lambdaE{y}{\kappa~(x,\, y)})})}, \]
which is precisely the way a tuple of expressions would be evaluated in CPS.

You might see where this is going: we can implement the delimited continuation operators, shift and
reset, as operations in the continuation monad! We can define
\begin{center}
  \begin{tabular}{lll}
    $\textsf{shift}$~$f$ & $=$ & $\lambdaE{\kappa}{f~(\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}})~(\lambdaE{x}{x})}$ \\
    $\textsf{reset}$~$\overline{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~(\overline{x}~(\lambdaE{x}{x}))}$
  \end{tabular}
\end{center}
and then we can implement examples from \citeauthor{danvy1989functional} like
\[
  [x + y \mid x \leftarrow [1];\ y \leftarrow \textsf{reset}~[u + v \mid u \leftarrow [10];\ v \leftarrow \textsf{shift}(\lambdaE{k}{[b \mid a \leftarrow k~100;\ b \leftarrow k~a]})]]. \\
\]
Admittedly, this is not as terse as
\[ 1 + \reset{10 + \shift{k}{k~(k~100)}}, \]
but it says and does the same thing, and critically neither version relies on manually managing
continuation parameters.

We can tie everything together by inspecting the results of Wadler's call-by-value translation into
the continuation monad.
\begin{align*}
  x^* &= [x]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~x} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{e^*})} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{f}{e_2^*~(\lambdaE{x}{f~x~\kappa})})} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{x}{e_2^*~(\lambdaE{y}{\kappa (x,\, y)})})} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{Cont} &= \lambdaE{\kappa}{e^*~(\lambdaE{x}{\textsf{\color{ACMDarkBlue} fst}~x)}}
\end{align*}
This translation is exactly the first CPS translation from Section \ref{sec:danvy}! And if we
extend the transformation to include our embeddings of shift and reset into the continuation
monad,
\begin{align*}
  (\shift{k}{e})^* &= \textsf{shift}~(\lambdaE{k}{e^*}) &= (\lambdaE{\kappa}{e^*~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \reset{e}^* &= \textsf{reset}~e^* &= \lambdaE{\kappa}{\kappa~(e^*~\textsf{id})},
\end{align*}
we arrive at the ECPS translation.

Wadler did point out the former observation---in fact, he also observed that the call-by-name
translation results in the unusual call-by-name version of the CPS translation---but I was very
pleased to complete the picture and add translations for shift and reset. Of course, the shift
and reset translations mean that this extension of $(\cdot)^*$ is no longer generic over the
choice of monad, but the same goes for the translations of any monad-specific operations (e.g.
get and put).

\section{Conclusion} \label{sec:conclusion}

\outline{
  \begin{itemize}
    \item Some related work
    \item Mention algebraic effects
  \end{itemize}
}

\begin{acks}
\end{acks}

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}

% \begin{quote}
%   [In some languages,] it may be possible to jump out of an expression and then later jump back
%   into it again and resume the process of evaluation. Continuations are sufficiently powerful to
%   deal with such a situation. (This could not be taken to imply approval of jumps back into
%   expressions as a language design feature—but if a language can specify something, however odd,
%   the method used to give its formal semantics must be powerful enough to describe it.)
%   \cite{strachey2000continuations}
% \end{quote}
