\documentclass[acmsmall, nonacm, screen]{acmart}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{anyfontsize}

\newif\ifdraft\drafttrue

\definecolor{green}{HTML}{298a33}
\definecolor{orange}{HTML}{995d02}

\newcommand{\outline}[1]{
  \ifdraft
  {\color{red}{#1}}
  \fi
}

\newcommand\doverline[1]{%
  \setbox0=\hbox{$\overline{#1}$}%
  \ht0=\dimexpr\ht0-.30ex\relax% CHANGE .15 TO AFFECT SPACING
  \overline{\copy0}%
}

\newcommand{\ifThenElse}[3]{\textsf{\color{ACMDarkBlue}if}~#1~\textsf{\color{ACMDarkBlue}then}~#2~\textsf{\color{ACMDarkBlue}else}~#3}
\newcommand{\caseOf}[2]{\textsf{\color{ACMDarkBlue} case}~#1~\textsf{\color{ACMDarkBlue}of}~\{~#2~\}}
\newcommand{\letIn}[3]{\textsf{\color{ACMDarkBlue}let}~#1 = #2~\textsf{\color{ACMDarkBlue}in}~#3}
\newcommand{\shift}[2]{\textsf{\color{ACMDarkBlue}shift}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\callcc}[2]{\textsf{\color{ACMDarkBlue}call/cc}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\reset}[1]{\langle #1 \rangle}
\newcommand{\lambdaE}[2]{\lambda #1.\, #2}
\newcommand{\just}[1]{\textsf{Just}~#1}
\newcommand{\nothing}{\textsf{Nothing}}
\newcommand{\map}[3]{\textsf{map}^{\textsf{#1}}~#2~#3}
\newcommand{\unit}[2]{\textsf{unit}^{\textsf{#1}}~#2}
\newcommand{\join}[2]{\textsf{join}^{\textsf{#1}}~#2}
\newcommand{\cps}[1]{\mathcal{C}\llbracket #1 \rrbracket}
\newcommand{\cpsm}[1]{\mathcal{C}'\llbracket #1 \rrbracket}
\newcommand{\cpsmc}[1]{\mathcal{C}''\llbracket #1 \rrbracket}
\newcommand{\denote}[1]{\mathcal{E}\llbracket #1 \rrbracket}
\newcommand{\stringE}[1]{\textsf{\color{green} ``#1''}}
\newcommand{\quoteE}[1]{{\color{orange} \ulcorner #1 \urcorner}}
\newcommand{\unquoteE}[1]{{\color{black} \llparenthesis #1 \rrparenthesis }}

\lstset{ %
  backgroundcolor=\color{white},
  commentstyle=\color{ACMGreen},
  keywordstyle=\color{ACMDarkBlue},
  stringstyle=\color{ACMPurple},
  basicstyle=\ttfamily
}

\lstdefinestyle{hs}{
  language=Haskell
}

\lstdefinestyle{rkt}{
  language=lisp,
  deletekeywords={get},
  morekeywords={define},
  literate=*{(}{{\textcolor{gray}{(}}}{1}
    {)}{{\textcolor{gray}{)}}}{1}
}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\acmBooktitle{}

\begin{document}

\title{Delimited Continuations and Monads}
\subtitle{A Comparison of Programming Abstractions}
\titlenote{
  This report was compiled for part of the University of Pennsylvania's WPE-II Exam. The
  accompanying talk is available on the author's website.
}

\author{Harrison Goldstein}
\email{hgo@seas.upenn.edu}
\orcid{0000−0001−9631−1169}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia, PA}
  \country{USA}
}

\renewcommand{\shortauthors}{Goldstein}

\begin{abstract}
  In 1990, two programming abstractions were introduced independently: {\em delimited
  continuations} and {\em monads}. \citet{danvy1990abstracting} explored delimited continuations
  as a principled way of manipulating program contexts as first-class functions.
  \citet{wadler1990comprehending} popularized monads as a tool for simulating effects in a pure
  language. Though they initially seem to have little to do with one-another, delimited
  continuations and monads actually have a lot in common: they can implement many of the same
  design patterns, and their meta-theories are surprisingly compatible. This report
  re-contextualizes those early 90s papers and explores the commonalities between monads and
  delimited continuations.
\end{abstract}

\maketitle

\section{Introduction} \label{sec:introduction}
When writing an algorithm, programmers want to focus on the interesting parts and ignore the
implementation details. \outline{Rewrite, recalibrate for audience} Error cases can be neatly
packaged up in exceptions, mutable state can be implemented with references, and
non-deterministic choices can be made by random number generators, all while the programmer
focuses on the important parts of their algorithm. Unfortunately, languages with built-in {\em
effects} like errors, state, and nondeterminism are often harder to reason about than
computationally {\em pure} languages without such features. Is it possible to abstract away
effects {\em and} keep the benefits of purity? Two abstractions, developed independently in the
early 1990s, provide ways to do just that.

Consider the following programs that perform simple error handling:
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=rkt, deletekeywords={abort}, mathescape]
  (define (p n)
    (let ([i 84]
          [j (if (= n 0)
               (abort)
               n)])
      (/ i j)))

  (run ($\lambda$ () (p 2)) ; '('Just 42)
  (run ($\lambda$ () (p 0)) ; 'Nothing
  \end{lstlisting}
  \label{fig:racket-state}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=hs]
  p n = do
    i <- unit 84
    j <- if n == 0
      then abort
      else unit n
    unit (i `div` j)

  p 2 -- Just 42
  p 0 -- Nothing
  \end{lstlisting}
  \label{fig:hs-state}
\end{subfigure}
\Description[Maybe Abstraction]{Code that might throw an exception.}
\label{fig:stateful-comps}
\end{figure}
\noindent Both of these programs are pure---they do not use exceptions or other built-in effects.
The error handling is done entirely by underlying abstractions that allow the programmer to think
in terms of exceptions without adding native effects to the language.

The first snippet, written in Racket, uses {\em delimited continuations} to implement error
handling. A continuation is a first-class representation of the ``rest'' of a computation. For
example, when evaluating the sub-expression ``$2$'' in a simplified version of our above example,
\[ \letIn{i}{84}{\letIn{j}{2}{i / j}}, \]
the remaining {\em context} is
\[ \letIn{i}{84}{\letIn{j}{\_}{i / j}}, \]
and the continuation, $k$, is
\[ k = \lambdaE{x}{\letIn{i}{84}{\letIn{j}{x}{i / j}}}. \]
The continuation captures the context as a function. In {\em Abstracting Control},
\citet{danvy1990abstracting} present two operators for manipulating continuations: {\em shift}
(written ``$\shift{k}{e}$'') and {\em reset} (written ``$\reset{e}$''). \outline{Reset delimits
the scope of the current context---in the example above, we implicitly wrap the whole expression
in a reset. Degenerate} The expression ``$\shift{k}{e}$'' captures the current context as a
continuation, $k$, and then executes $e$ in a fresh context. We can use shift and reset to
extract the continuation from above:
\begin{align*}
  & \reset{\letIn{i}{84}{\letIn{j}{(\shift{k}{k~2})}{i / j}}} \\
  \rightarrow\ & \letIn{k}{(\lambdaE{x}{\letIn{i}{84}{\letIn{j}{x}{i / j}}})}{k~2}
\end{align*}
Reset delimits the context, in this case the whole expression, and shift packages the context as
$k$. In this example the shift body just applies $k$, so nothing has changed---the result will be
the same as the original expression. But the shift body can be anything. In particular, if $k$ is
not used at all
\[ \reset{\letIn{i}{84}{\letIn{j}{(\shift{k}{\textsf{Nothing}})}{i / j}}} \]
the result is essentially an {\em exception}: the expression is thrown away and the result of the
computation is just \textsf{Nothing}. This is roughly how the Racket example above is
implemented.

The second snippet simulates error-handling computations using a {\em monad} in Haskell. Monads
were introduced to the programming languages literature by \citet{moggi1991notions}, but they
were popularized by Wadler in {\em Comprehending Monads}~\cite{wadler1990comprehending}. As a
programming abstraction, monads generalize the well-known idea of list comprehensions:
\[
  [ (x,\,y) \mid x \leftarrow [1,\, 2] ;\ y \leftarrow [3,\, 4] ]
\]
This expression results in the Cartesian product of lists ``$[1,\, 2]$'' and ``$[3,\, 4]$.''
Wadler observed that comprehension syntax can be used to program with any monad (not just
$\textsf{List}$). For example:
\[
  [ i / j \mid i \leftarrow [84]^{\textsf{Maybe}} ;\ j \leftarrow \ifThenElse{n = 0}{\textsf{Nothing}}{[n]^{\textsf{Maybe}}} ]^{\textsf{Maybe}}
\]
This monad, usually called $\textsf{Maybe}$, is another way to implement pure exceptions: if the
result of comprehension will be ``$\nothing$'' if $n = 0$, otherwise the result will be
``$\just{42}$.'' There are dozens of monads that programmers use in practice, each of which gives
a different interpretation of comprehension syntax.

It is not immediately clear that manipulating continuations has anything to do with interpreting
comprehension syntax. One would be forgiven for assuming that these ideas are largely orthogonal.
But we have already seen that that delimited continuations and monads can both be used to
simulate exceptions. Could there be more overlap? Yes! It turns out that that all of the effects
we discussed before, and a number of others, can be implemented using either delimited
continuations or monads. Furthermore, continuations actually form a monad, and Wadler shows that
this fact can be exploited to recover results from Danvy and Filinski.

In this report I make three contributions:
\begin{itemize}
  \item I summarize and re-contextualize two influential papers: {\em Abstracting Control} by
  Danvy and Filinski (\S~\ref{sec:danvy}) and {\em Comprehending Monads} by
  Wadler (\S~\ref{sec:wadler}).\footnote{This is a requirement of
  the WPE-II exam, but I hope that reformulating these results will still be instructive.}
  \item I show many design patterns that can be implemented equally well by both delimited
  continuations and monads, establishing a close relationship between the two language features
  (\S~\ref{sec:patterns}).
  \item I explore the {\em continuation monad} and show that, by performing a monad-agnostic
  transformation given by Wadler, we can recover the ``extended continuation passing style''
  transformation presented by Danvy and Filinski (\S~\ref{sec:contmonad}). This is
  a small extension of Wadler's original result.
\end{itemize}
I conclude with some remarks on the broader context of these abstractions (\S~\ref{sec:conclusion}).

\subsection*{Unified Notation} \label{sec:notation}
My two source papers differ slightly on notations and conventions. To aid with comprehension, I
have chosen a reasonable middle-ground and use one unified set of conventions to discuss both
papers. Throughout this report, assume that programs are written in a lambda calculus given by
the grammar
\begin{align*}
  e \Coloneqq &\ x \mid \lambdaE{x}{e} \mid e_1~e_2 \\
             | &\ \textsf{\color{ACMDarkBlue} true} \mid \textsf{\color{ACMDarkBlue} false} \mid \ifThenElse{e_1}{e_2}{e_3} \\
             | &\ (e_1,\, e_2) \mid \textsf{\color{ACMDarkBlue} fst}~e \mid \textsf{\color{ACMDarkBlue} snd}~e.
\end{align*}
Occasionally I will ignore either Booleans or pairs for brevity. Since we are almost exclusively
concerned with the dynamic semantics of these programs, I will not formalize a type system.

\section{{\em Abstracting Control}} \label{sec:danvy}
Before explaining Danvy and Filinski's contributions, I should explain some background on
continuations and first-class continuation operators. Continuations first appeared as
meta-theoretic tools \outline{cite}, but before long they were introduced into concrete languages
via the {\em continuation-passing style} or CPS translation. Here is a call-by-value CPS
translation, $\cps{\cdot}$, for our basic lambda calculus with Booleans and if-statements:
\begin{align*}
  \cps{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cps{\lambdaE{x}{e}} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{\cps{e}})} \\
  \cps{e_1~e_2} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \\
  \cps{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cps{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{b}{\ifThenElse{b}{\cps{e_2}~\kappa}{\cps{e_3}~\kappa}})}
\end{align*}
Translated terms do not ``return'' in the traditional sense. When these terms finish computing,
they pass the resulting value to a continuation, $\kappa$, which is provided as an argument. 
A translated term can be run by passing the trivial continuation: $\cps{e}~(\lambdaE{x}{x})$.

The CPS translation has been used extensively in compilers and interpreters, where it gives
fine-grained control over evaluation. This is because the translation makes evaluation order
explicit---this particular CPS translation is clearly call-by-value, since in the rule for
application,
\[ \cps{e_1~e_2} = \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \]
the argument $e_2$ is evaluated before $f$ is applied.

Once the CPS translation became established researchers began to consider first-class
abstractions for working with continuations. The most famous of these early abstractions, ``call
with current continuation'' (or ``call/cc'') \outline{cite}, can be added as an operator in our
source language and implemented as an extension to the CPS translation:
\[ \cps{\callcc{k}{e}} = (\lambdaE{\kappa}{\cps{e}~\kappa})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa~x}}], \]
where ``$e[x \mapsto v]$'' means $e$ with $v$ substituted for free instances of $x$.

The call/cc operator is extremely powerful. The expression $\callcc{k}{e}$ captures the current
program context as $k$ and then runs $e$, allowing the programmer to arbitrarily pause and resume
evaluation as they see fit. For example,
\[ \callcc{k}{1} \]
aborts the computation, returning the value $1$, and
\[ \callcc{k}{(\textsf{\color{ACMDarkBlue} print}~\stringE{foo};\ k~10)} \]
pauses the computation, prints ``foo'', and then resumes computation with the value 10.
Unfortunately, many have said that call/cc is {\em too} powerful. Consider the popular Yin-Yang
Puzzle:
\begin{align*}
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yin}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{@};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yang}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{*};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{yin}~\textsf{yang}
\end{align*}
What does this confusing mess of continuations do? Apparently it counts. This program prints
increasing numbers in a unary representation,
\[ \stringE{@*@**@***@****@*****...}, \]
but it is extremely difficult to understand exactly why that is.

\citet{felleisen1988theory} attempted to reign in the power of call/cc with operators that he
called ``prompt'' and ``control''. These allowed programmers to delimit the effects of call/cc
with {\em scopes}. Unfortunately, Felleisen's scopes were dynamic, and his approach did not admit
a straightforward translation into a standard lambda calculus. This was the impetus for Danvy and
Filinski's statically delimited continuations.

\subsection{Delimited Continuations}
In {\em Abstracting Control}, Danvy and Filinski introduce the ``shift'' and ``reset'' operations
mentioned in Section \ref{sec:introduction}, which provide a more usable alternative to call/cc.
\[
  e \Coloneqq \cdots \mid \shift{k}{e} \mid \reset{e}
\]
These operators can also be interpreted via a modified CPS translation that Danvy and Filinski
call {\em extended continuation-passing style} (ECPS):
\begin{align*}
  \cps{\shift{k}{e}} &= (\lambdaE{\kappa}{\cps{e}~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \cps{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cps{e}~\textsf{id})}
\end{align*}
\outline{explain}
Shift and reset provide a more intuitive way of working with continuations. Reset delimits the
current context, and shift gives programmatic access to that context. For example, take the
following expression and its evaluation:
\begin{align*}
& 1 + \reset{10 + \shift{k}{k~(k~100)}} \Rightarrow \\
& 1 + (10 + (10 + 100)) \Rightarrow \\
& 121
\end{align*}
The continuation ``$k = \lambdaE{x}{10 + x}$'' captured by the shift operator, since ``$10 +
\shift{k}{\dots}$'' is within a context delimited by reset. The function $k$ is applied twice to
$100$, resulting in $121$, and then we add $1$, resulting in $121$.

In addition to the ECPS translation, Danvy and Filinski give a denotational semantics for the
lambda calculus extended with shift and reset. Assuming $\textsf{Ans}$ is a suitable domain of
final answers, we define
\begin{center}
  \begin{tabular}{llr}
    $\rho \in \textsf{Env}$ & $=$ & $\textsf{Var} \rightharpoonup \textsf{Val}$ \\
    $\gamma \in \textsf{MCont}$ & $=$ & $\textsf{Val} \to \textsf{Ans}$ \\
    $\kappa \in \textsf{Cont}$ & $=$ & $\textsf{Val} \to \textsf{MCont} \to \textsf{Ans}$ \\
    $\mathcal{E}$ & $:$ & $\textsf{Exp} \to \textsf{Env} \to \textsf{Cont} \to \textsf{MCont} \to \textsf{Ans}$.
  \end{tabular}
\end{center}
\outline{More here?}
Intuitively $\rho$ is the variable environment, $\gamma$ is the meta-continuation, and $\kappa$
is the continuation.
The denotational semantics is given by the equations
\begin{align*}
  \denote{x}~\rho~\kappa~\gamma &= \kappa~(\rho[x])~\gamma \\
  \denote{\lambdaE{x}{e}}~\rho~\kappa~\gamma &= \kappa~(\lambdaE{v}{\denote{e}~(\rho[x \mapsto v])})~\gamma \\
  \denote{e_1~e_2}~\rho~\kappa~\gamma &=
    \denote{e_1}~\rho~(\lambdaE{f}{\denote{e_2}~\rho~(\lambdaE{x}{f~x~\kappa})})~\gamma \\
  \denote{\textsf{\color{ACMDarkBlue}true}}~\rho~\kappa~\gamma &= \kappa~\textsf{\color{ACMDarkBlue} true}~\gamma \\
  \denote{\ifThenElse{e_1}{e_2}{e_3}}~\rho~\kappa~\gamma &= 
    \denote{e_1}~\rho~(\lambdaE{b}{\ifThenElse{b}{\denote{e_2}~\rho~\kappa}{\denote{e_3}~\rho~\kappa}})~\gamma \\
  \denote{\shift{k}{e}}~\rho~\kappa~\gamma &=
    \denote{e}~(\rho[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\lambdaE{\gamma'}{\kappa~x~(\lambdaE{w}{\kappa'~w~\gamma'})}}}])~(\lambdaE{x}{\lambdaE{\gamma''}{\gamma''~x}})~\gamma \\
  \denote{\reset{e}}~\rho~\kappa~\gamma &= \denote{e}~\rho~(\lambdaE{x}{\lambdaE{\gamma'}{\gamma'~x}})~(\lambdaE{x}{\kappa~x~\gamma})
\end{align*}
where constructions on the right-hand side are understood as syntax of a meta-language, rather
than as concrete syntax. Note that some $\gamma$ arguments \outline{BCP: where?} are
$\eta$-reduced away to simplify presentation---in fact, they can be entirely elided for all rules
other than the ones for shift and reset.

\outline{Don't like this paragraph} This denotational semantics is quite similar in form to the
ECPS translation but the meta-continuation $\gamma$ gives a bit more intuition about what is
going on with shift and reset that is different from call/cc.
\[ 
  \denote{\callcc{k}{e}}~\rho~\kappa~\gamma =
    \denote{e}~(\rho[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa~x}}])~\kappa~\gamma \\
\]
Shift and reset use the meta-continuation to keep track of the continuation scope, which makes it
easier to avoid the confusing ``action at a distance'' that call/cc sometimes creates.

\subsection{Metacircular Interpreters}
A {\em metacircular interpreter} is a powerful tool for expressing a language's
semantics~\cite{reynolds1972definitional}. Rather than translate code into abstract objects or
describe it via operational rules, a metacircular interpreter defines a language by translating
its constructs into the equivalent constructs in a well-understood meta-language. Danvy and
Filinski use this approach to build a more efficient ECPS translation.

For this interpreter, our meta-language is the same lambda calculus we have been working with,
but without shift or reset and with a very simple {\em quoting} mechanism. In the following
program, the code in black is the interpreter, and it produces $\quoteE{\text{quoted}}$ code as
an output. We use $\unquoteE{\text{unquote brackets}}$ to splice computations into quoted
segments.
\begin{align*}
  \cpsm{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cpsm{\lambdaE{x}{e}} &=
    \lambdaE{\kappa}{\kappa~\quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{k}{\unquoteE{\cpsm{e}~(\lambdaE{a}{\quoteE{k~\unquoteE{a}}})}}}}} \\
  \cpsm{e_1~e_2} &= \lambdaE{\kappa}{\cpsm{e_1}~(\lambdaE{f}{\cpsm{e_2}~(\lambdaE{x}{\quoteE{\unquoteE{f}~\unquoteE{x}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}})})} \\
  \cpsm{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cpsm{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cpsm{e_1}~(\quoteE{\lambdaE{b}{\ifThenElse{b}{\unquoteE{\cpsm{e_2}~\kappa}}{\unquoteE{\cpsm{e_3}~\kappa}}}})} \\
  \cpsm{\shift{k}{e}} &= (\lambdaE{\kappa}{\cpsm{e}~\textsf{id}})[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}] \\
  \cpsm{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cpsm{e}~\textsf{id})}
\end{align*}
\outline{too fast}
The final ECPS result is given by ``$\cpsm{e}~\textsf{id}$''. This optimized translation avoids
inserting unnecessary redexes, and produces a much simpler ECPS result than the original
translation.

It turns out that we can do even better if we include delimited continuation operators in our
meta-language. This final ECPS translation, 
\begin{align*}
  \cpsmc{x} &= x \\
  \cpsmc{\lambdaE{x}{e}} &= \quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{\kappa}{\unquoteE{\reset{\quoteE{\kappa~\unquoteE{\cpsmc{e}}}}}}}} \\
  \cpsmc{e_1~e_2} &= \shift{\kappa}{\quoteE{\unquoteE{\cpsmc{e_1}}~\unquoteE{\cpsmc{e_2}}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}} \\
  \cpsmc{\textsf{\color{ACMDarkBlue}true}} &= \textsf{\color{ACMDarkBlue}true} \\
  \cpsmc{\ifThenElse{e_1}{e_2}{e_3}} &= \shift{\kappa}{\quoteE{\ifThenElse{\unquoteE{\cpsmc{e_1}}}{\unquoteE{\reset{\kappa~\cpsmc{e_2}}}}{\unquoteE{\reset{\kappa~\cpsmc{e_3}}}}}} \\
  \cpsmc{\shift{k}{e}} &= \shift{\kappa}{\reset{\cpsmc{e}}[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}]} \\
  \cpsmc{\reset{e}} &= \reset{\cpsmc{e}}
\end{align*}
also avoids unnecessary $\eta$-expansion, resulting in a pleasingly compact final representation.

This final interpreter is essentially written in the same language that it interprets---this is
why metacircular interpreters are called ``meta'' and it does pose a bootstrapping problem.
Luckily, we already have both the ECPS translation and our denotational semantics as descriptions
of this meta-language. The efficient interpreter can be written in this meta-language and then
that interpreter can itself be ECPS transformed into a standard lambda calculus.

\subsection{Use Case: Nondeterministic Programming} \label{sec:danvy:nondet}

Delimited control operators can be used to solve complex programming problems. Danvy and Filinski
choose to highlight one application in particular: nondeterministic programming. The nondeterministic
operators \textsf{fail} and \textsf{flip} are defined first:
\begin{align*}
\textsf{fail}~() &= \shift{k}{\stringE{failure}} \\
\textsf{flip}~() &= \shift{k}{(k~\textsf{\color{ACMDarkBlue}true};\ k~\textsf{\color{ACMDarkBlue}false};\ \textsf{fail}~())}
\end{align*}
The \textsf{fail} operation just acts as an exception, throwing away the continuation and
returning the string $\stringE{failure}$. The main source of nondeterminism is \textsf{flip}---it
actually calls its continuation on {\em both} \textsf{\color{ACMDarkBlue}true} and
\textsf{\color{ACMDarkBlue}false}, but the caller is expected to use \textsf{flip} as if it
nondeterministically chooses between the two options (in model of nondeterminism, those two
points of view coincide). Finally, \textsf{choice} builds on \textsf{flip} to simulate a
nondeterministic choice of integers less than a given value.
\[
  \textsf{choice}~n = \ifThenElse{n < 1}{\textsf{fail~()}}{\ifThenElse{\textsf{flip}~()}{\textsf{choice}~(n - 1)}{n}}
\]

Danvy and Filinski use these operators to implement
\begin{align*}
\textsf{triple}~n~s =&  \\
& \textsf{\color{ACMDarkBlue}let}~i = \textsf{choice}~n~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~j = \textsf{choice}~(i - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~k = \textsf{choice}~(j - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \ifThenElse{i + j + k = s}{(i,\, j,\, k)}{\textsf{fail}~()},
\end{align*}
which finds all triples of distinct positive integers that sum to a given integer $s$. Evaluating
the expression ``$\reset{\textsf{\color{ACMDarkBlue}print}~(\textsf{triple}~9~15)}$'' prints all
triples of integers up to $9$ that sum to $15$.

The \textsf{triple} example is mostly a toy, but these primitives can be practically useful. In
the paper, Danvy and Filinski show an evaluator for {\em nondeterministic finite automata} (NFAs),
which is straightforward to implement thanks to operators like \textsf{flip} and \textsf{fail}.
The implementation is not especially instructive, so I will not repeat it here.


% \subsection{Generalizing to More Contexts}

% The final contribution in {\em Abstracting Control} is an idea for a {\em family} of shift and
% reset operators that operate on an indexed family of program contexts. They introduce operators
% \begin{center}
%   $\textsf{\color{ACMDarkBlue}shift}_i~k~\textsf{\color{ACMDarkBlue}in}~e$ \hspace{5mm}
%   and \hspace{5mm} $\reset{e}_i$
% \end{center}
% which access and delimit the $i$th enclosing context respectively. The goal here is to be able to
% associate different simulated effects (e.g. error handling and nondeterminism) with different
% indexed operators; without such a mechanism the effects do not compose well.

% These operators can also be given meaning via the ECPS translation, but with a bit more hassle. A
% program that uses indices up to $n$ will need to be translated $n + 1$ times. The translations
% are defined by mutual induction on term structure (as before) and on operator indices.
% \begin{align*}
%   \cps{\textsf{\color{ACMDarkBlue}shift}_0~k~\textsf{\color{ACMDarkBlue}in}~e} &= e[k \mapsto \textsf{id}] \\
%   \cps{\textsf{\color{ACMDarkBlue}shift}_{n + 1}~k~\textsf{\color{ACMDarkBlue}in}~e} &= 
%     \lambdaE{\kappa}{\textsf{\color{ACMDarkBlue}shift}_{n}~k'~\textsf{\color{ACMDarkBlue}in}~(\cps{e}~\textsf{id})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\reset{k'~(\kappa~x)}_n}}]} \\
%   \cps{\reset{e}_0} &= e \\
%   \cps{\reset{e}_{n + 1}} &= \lambdaE{\kappa}{\kappa~\reset{\cps{e}~\textsf{id}}_n}
% \end{align*}
% See Danvy and Filinski's paper for more of the meta-theory for these constructs.

Now that we have a clear picture of delimited continuation operators, we can move on to monads.

\section{{\em Comprehending Monads}} \label{sec:wadler}

Monads were originally introduced to the programming languages literature by
\citet{moggi1991notions}, and {\em Comprehending Monads} by \citet{wadler1990comprehending}
helped to popularize them. A monad is a mathematical structure commonly used in category theory.
There, a monad is defined as a {\em functor} with certain associated operations that obey a set
of laws.

For our purposes, a {\em functor} is a type constructor (e.g. \textsf{List}, \textsf{Maybe}) with
an operation \textsf{map} that applies a function ``under'' the type constructor. For example,
\[ \map{List}{f}{\overline{x}} \]
applies $f$ to all elements of $\overline{x}$ and 
\[ \map{Maybe}{f}{\overline{x}} \]
applies $f$ to the value contained in $\overline{x}$, if one exists. (I adopt Wadler's convention
of writing monadic values---values wrapped in the monad's type constructor---with a bar over the
variable name.) Map must obey two laws:
\begin{align*}
  \textsf{map}~\textsf{id} &= \textsf{id} \\
  \textsf{map}~(g \circ f) &= \textsf{map}~g \circ \textsf{map}~f
\end{align*}
The first law says that mapping the identity function over a structure does nothing---this is
necessary to make sure that \textsf{map} does not change the structure itself, only the elements
in the structure. The second law says that \textsf{map} is well behaved with respect to function
composition.

In order for a functor to be a monad, it needs two more operations: \textsf{unit} and \textsf{join}.
Applying \textsf{unit} to a value injects that value into the monad. In the case of \textsf{List},
\[ \unit{List}{x} = \textsf{singleton}~x = [x], \]
which is the simplest way to inject a value into a list. Likewise,
\[ \unit{Maybe}{x} = \textsf{Just}~x. \]

The \textsf{join} operation works on ``doubled up'' instances of the monad. It takes a doubled
value like
\begin{center}
  ``$[[1, 2], [3, 4]]$''\hspace{5mm}or\hspace{5mm}``$\textsf{Just}~(\textsf{Just}~5)$''
\end{center}
and flattens it down to a single application of the monad like
\begin{center}
  ``$[1, 2, 3, 4]$''\hspace{5mm}or\hspace{5mm}``$\textsf{Just}~5$''.
\end{center}
To be precise,
\begin{center}
  \begin{tabular}{lll}
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
\end{center}
Functors with a \textsf{join} operation admit a special kind of function chaining called {\em
Kleisli composition}. Given a functor $F$ and two functions $f: \beta \to F~\gamma$ and $g: \alpha
\to F~\beta$, we can write
\[ f \circ_F g = \textsf{join}^F \circ map^F~f \circ g \]
that composes them together while properly keeping track of $F$. This is useful in programming
because it means that monadic operations like $f$ and $g$ can be built up individually and
composed together to build larger programs. A very similar construction can also yield the
``\textsf{bind}'' (or ``\textsf{>>=}'') definition of monads that is more common in modern
programming languages. Though these alternative views of monads are probably more useful for
programming, they are slightly harder to work with for theory, so I will continue to use Wadler's
notation.

These operations must obey certain laws in order for the structure to truly be a monad:
\begin{align}
  \textsf{map}~f \circ \textsf{unit} &= \textsf{unit} \circ f \\
  \textsf{map}~f \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~(\textsf{map}~f) \\
  \textsf{join} \circ \textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{map}~\textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~\textsf{join}
\end{align}
Intuitively, laws (1) and (2) show that \textsf{unit} and \textsf{join} are ``well-behaved'' with
respect to \textsf{map}, and laws (3-5) require that \textsf{unit} and \textsf{join} are the {\em
simplest} ways to do their respective tasks. For example, suppose we tried to implement
\[ \unit{List}{x} = [x,\, x,\, x,\, x,\, x]. \]
Technically this does inject a value into a list, but it is not the simplest way to do so and
\[
  (\textsf{join}^{\textsf{List}} \circ \textsf{unit}^{\textsf{List}})~[3] = [3,\, 3,\, 3,\, 3,\, 3] \neq \textsf{id}~[3]. \\
\]
Likewise, the laws require that \textsf{join} flattens a stack of monads in a simple, predictable
way.

All of this background on monads and their operations is important, but in practice using these
operations directly does not make for particularly elegant code. In the next section, I explore
Wadler's use of {\em comprehensions} to make programming with monads much more intuitive.

\subsection{Comprehensions}
List comprehensions are based on set-builder notation (e.g. $\{n + 1 \mid n \in \mathbb{N}\}$)
and were first introduced to programming by the SETL language in the
1960s~\cite{schwartz2012programming}. Since then, list comprehensions have become ubiquitous,
appearing in many modern languages including Racket, Python, and even C++ (with some abuse of
operator overloading). Comprehensions are a convenient way of building lists from other lists,
for example a construction like
\[ [(x,\, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{List}} \]
computes the Cartesian product of $\overline{x}$ and $\overline{y}$. The equivalent set-builder
construction looks like
\[ \{(x,\, y) \mid x \in X \wedge y \in Y\}. \]

Wadler starts off by adding a formal syntax for list comprehensions to a standard lambda calculus:
\begin{align*}
 e &\Coloneqq \cdots \mid [e \mid q] \\
 q &\Coloneqq \Lambda \mid x \leftarrow e \mid (q_1;\ q_2)
\end{align*}
The symbol $q$ stands for {\em qualifiers}, which can be empty (denoted $\Lambda$), a binder for
a variable $x$, or a composition of two qualifiers. List comprehension syntax is defined by the
following rules:
\begin{align*}
  [e \mid \Lambda] &= \textsf{singleton}~e \\
  [e_1 \mid x \leftarrow e_2] &= \textsf{map}~(\lambdaE{x}{e_1})~e_2 \\
  [e \mid (q_1;\ q_2)] &= \textsf{flatten}~[[e \mid q_2] \mid q_1]
\end{align*}
This notation can be a bit heavy, so Wadler usually writes ``$[e]$'' instead of ``$[e \mid
\Lambda]$,'' and he proves that qualifier composition is associative so he can write ``$[e \mid
q_1;\ q_2]$'' instead of $[e \mid (q_1;\ q_2)]$.

{\em Comprehending Monads} centers around the observation that lists are not the only structure
for which comprehensions make sense. Since \textsf{singleton} is $\textsf{unit}^{\textsf{List}}$
and \textsf{flatten} is $\textsf{join}^{\textsf{List}}$, we can try to re-interpret
comprehensions for any monad. Wadler observed that every monad \textsf{M} has an associated monad
comprehension given by the following rules:
\begin{align*}
  [e \mid \Lambda]^{\textsf{M}} &= \unit{M}{e} \\
  [e_1 \mid x \leftarrow e_2]^{\textsf{M}} &= \map{M}{(\lambdaE{x}{e_1})}{e_2} \\
  [e \mid (q_1;\ q_2)]^{\textsf{M}} &= \join{M}{[[e \mid q_2]^{\textsf{M}} \mid q_1]^{\textsf{M}}} \\
\end{align*}
This is how we arrive at the \textsf{Maybe} comprehension from Section \ref{sec:introduction},
\[
  [ i / j \mid i \leftarrow [84]^{\textsf{Maybe}} ;\ j \leftarrow \ifThenElse{n = 0}{\textsf{Nothing}}{[n]^{\textsf{Maybe}}} ]^{\textsf{Maybe}}
\]
which cleanly handles the fact that the expression binding $j$ might fail, without introducing
significant control-flow overhead.

Monad comprehensions were the first step towards the ``do--notation'' that is popular in Haskell.
\[ \textsf{\color{ACMDarkBlue} do}\ \{\ i \leftarrow \unit{Maybe}{84};\ j \leftarrow
(\ifThenElse{n = 0}{\nothing}{\unit{Maybe}{n}});\ \unit{Maybe}{i/j}\ \}, \] The distinctions
between comprehension syntax and do--notation are mostly cosmetic. When using do--notation,
programmers write ``$\unit{Maybe}{w}$'' (or more idiomatically ``$\textsf{return}~e$'') instead
of $[e]^{\textsf{M}}$. Also rather than put a return value at the front of the comprehension,
do--blocks evaluate to their final expression. For the remainder of this paper I will stick to
comprehension syntax, but programming-focused monad examples usually use something closer to
do--notation.

\subsection{Examples of Monads} \label{sec:monad-examples}
Wadler presents a whole zoo of monads in his paper; exploring those structures provides valuable
intuition for how monads work and what they can do.

\subsubsection{List and Maybe}
These first two monads have been running examples in this section, so I will not spend much time
on them here. Both the \textsf{List} and \textsf{Maybe} types are defined as informal algebraic
data types, similar to how they would be defined in a language like Haskell. The \textsf{List}
monad gives rise to standard list comprehensions, and the \textsf{Maybe} monad can be used to chain
operations that might potentially fail.
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type List}$~$\alpha$ & $=$ & $\textsf{Nil} \mid \textsf{Cons}~\alpha~(\textsf{List}~\alpha)$ \\
    $\map{List}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nil} \to \textsf{Nil};\ \textsf{Cons}~y~\overline{y} \to \textsf{Cons}~(f~y)~(\map{List}{f}{\overline{y}})}$ \\
    $\unit{List}{x}$ & $=$ & $\textsf{singleton}~x$ \\
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$ \\
    \\
    $\textsf{type Maybe}$~$\alpha$ & $=$ & $\textsf{Nothing} \mid \textsf{Just}~\alpha$ \\
    $\map{Maybe}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nothing} \to \textsf{Nothing};\ \textsf{Just}~y \to \textsf{Just}~(f~y)}$ \\
    $\unit{Maybe}{x}$ & $=$ & $\textsf{Just}~x$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
  }
\end{center}
\vfill

\subsubsection{Identity and Strictness}
The simplest possible monad is the identity functor---it does nothing. This monad is almost not
worth mentioning, but it does have one cute use-case: comprehension syntax for the identity monad
can be used instead of \textsf{\color{ACMDarkBlue} let} binding. Rather than write
\begin{center}
  $\letIn{x}{e_1}{\letIn{y}{e_2}{e_3}}$ \hspace{5mm} we can write \hspace{5mm} $[e_3 \mid x \leftarrow e_1;\ y \leftarrow e_2]^{\textsf{Id}}$.
\end{center}
There is no real reason to prefer this syntax, so I will continue to use
$\textsf{\color{ACMDarkBlue} let}$, but it is a fun observation.

A slight variation on the identity monad, the {\em strictness} monad, does actually have some
interesting uses. Assuming a call-by-name base language, the strictness monad would provide
control over evaluation order by altering the definition of $\textsf{map}$ to force the
evaluation of the argument $\overline{x}$. Statements in a \textsf{Str} comprehension would
essentially use call-by-value semantics.
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type Id}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Id}{f}{\overline{x}}$ & $=$ & $f~\overline{x}$ \\
    $\unit{Id}{x}$ & $=$ & $x$ \\
    $\join{Id}{\doverline{x}}$ & $=$ & $\doverline{x}$ \\
    \\
    $\textsf{type Str}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Str}{f}{\overline{x}}$ & $=$ & $\ifThenElse{\overline{x} \neq \bot}{f~\overline{x}}{\bot}$ \\
    $\unit{Str}{x}$ & $=$ & $x$ \\
    $\join{Str}{\doverline{x}}$ & $=$ & $\doverline{x}$
  \end{tabular}
  }
\end{center}
\vfill

\subsubsection{State}
The \textsf{State} monad makes enables code that {\em looks} stateful, even though there are no
actual references under the hood. The state monad is defined above, assuming some state type
$\sigma$. As with many monads, the real magic comes from auxiliary operations:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\lambdaE{s}{(s,\, s)}$ \\
    $\textsf{put}~s'$ & $=$ & $\lambdaE{s}{(x,\, s')}$
  \end{tabular}
\end{center}
These can be used to write the expression
\[
  [k \mid i \leftarrow \textsf{get};\ \_ \leftarrow \textsf{put}~(i + 3);\ j \leftarrow
  \textsf{get};\ \_ \leftarrow \textsf{put}~(j * 7);\ k \leftarrow \textsf{get}]^{\textsf{State}}
\]
which simulates a stateful computation; it can be run to compute the value $42$ (technically
$(42,\, 42)$).
\outline{could say more}
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type State}$~$\alpha$ & $=$ & $\sigma \to (\alpha,\, \sigma)$ \\
    $\map{State}{f}{\overline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(x,\, s')}{\overline{x}~s}{(f~x,\, s')}}$ \\
    $\unit{State}{x}$ & $=$ & $\lambdaE{s}{(x,\, s)}$ \\
    $\join{State}{\doverline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(\overline{x},\, s')}{\doverline{x}~s}{\overline{x}~s'}}$
  \end{tabular}
  }
\end{center}
\vfill

\subsubsection{Reader} (Also known as {\em State Reader}.)
The \textsf{Reader} monad is a simplification of the \textsf{State} monad that does not allow the
state to be modified. Despite its relative simplicity, the \textsf{Reader} monad is quite
powerful. After defining the auxiliary operation,
\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\lambdaE{r}{r}$,
  \end{tabular}
\end{center}
\textsf{Reader} can be used to keep track of configuration values, environment information, and
any other static value that would otherwise be cumbersome to pass around explicitly. Readers are
also safer and more flexible than global variables or constants, since they are implemented as
functions and thus have a clearly delimited scope.
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type Reader}$~$\alpha$ & $=$ & $\rho \to \alpha$ \\
    $\map{Reader}{f}{\overline{x}}$ & $=$ & $\lambdaE{r}{f~(\overline{x}~r)}$ \\
    $\unit{Reader}{x}$ & $=$ & $\lambdaE{r}{x}$ \\
    $\join{Reader}{\doverline{x}}$ & $=$ & $\lambdaE{r}{(\doverline{x}~r)~r}$
  \end{tabular}
  }
\end{center}
\vfill

\subsubsection{Nondeterminism} (Also known as {\em Set}.)
The nondeterminism monad enables direct-style programming of nondeterministic algorithms. It
avoids the messiness of a random number generator \outline{fix this on rewrite} by keeping track
of all possible values at once. Together with auxiliary operators,
\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\varnothing$ \\
    $\textsf{flip}$ & $=$ & $\{\textsf{\color{ACMDarkBlue} true},\, \textsf{\color{ACMDarkBlue} false}\}$,
  \end{tabular}
\end{center}
this monad can be extremely useful. \outline{could say more}
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type ND}$~$\alpha$ & $=$ & $2^\alpha$ \\
    $\map{ND}{f}{\overline{x}}$ & $=$ & $\{f~x \mid x \in \overline{x}\}$ \\
    $\unit{ND}{x}$ & $=$ & $\{x\}$ \\
    $\join{ND}{\doverline{x}}$ & $=$ & $\bigcup \doverline{x}$
  \end{tabular}
  }
\end{center}
\vfill

\subsubsection{Parser}
Finally, here is a more complicated monad: \textsf{Parser}. Parsers can be seen as a combination
of two monads: \textsf{List} and \textsf{State}. Intuitively, the \textsf{String} argument is the
parser input, and the result is a list of potential parses, each is made up of a result of type
$\alpha$ and the \textsf{String} that remains after parsing.

There are a number of auxiliary operations that are useful for \textsf{Parser} (often called {\em
parser combinators}), including ``\textsf{satisfy}'' which only parses a character that matches a
given predicate and ``\textsf{many}'' which applies a parser multiple times. An amazing amount of
research has been done on the \textsf{Parser} monad and parser
combinators~\cite{hutton1996monadic, leijen2001parsec}, and it is still an active
area~\cite{willis2020staged}.
\begin{center}
  \framebox[\textwidth]{
  \begin{tabular}{lll}
    $\textsf{type Parser}$~$\alpha$ & $=$ & $\textsf{String} \to \textsf{List}~(\alpha,\, \textsf{String})$ \\
    $\map{Parser}{f}{\overline{x}}$ & $=$ & $\lambdaE{i}{[(f~x,\, i') \mid (x,\, i') \leftarrow \overline{x}~i]^{\textsf{List}}}$ \\
    $\unit{Parser}{x}$ & $=$ & $\lambdaE{i}{[(x, i)]^{\textsf{List}}}$ \\
    $\join{Parser}{\doverline{x}}$ & $=$ & $\lambdaE{i}{[(x,\, i'') \mid (\overline{x},\, i') \leftarrow \doverline{x}~i;\ (x,\ i'') \leftarrow \overline{x}~i']^{\textsf{List}}}$
  \end{tabular}
  }
\end{center}
\vfill

\subsection{Translation}
Since monads can be used to replicate a variety of effects in a pure way, it would be nice if
there was a way to translate an impure program into the equivalent monadic one. Wadler gives two
such translations, one for call-by-value and another for call-by-name. The result of both
translations is a pure program in our lambda calculus extended with comprehensions.

The call-by-value translation ``$e^*$'' lifts a program of type $\alpha \to \beta$ to one of type
$\alpha \to \textsf{M}~\beta$ for some monad \textsf{M}. Intuitively, the translated function
takes a value of type $\alpha$ and returns a {\em computation} of type $\beta$. The monad
captures the effects of that computation. Here is the translation in full:
\begin{align*}
  x^* &= [x]^\textsf{M} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{M} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{M} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{M}
\end{align*}
Each of these rules has a straightforward computational meaning. For example
\[ (e_1~e_2)^* = [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \]
says that in order to evaluate a (potentially effectful) application, we evaluate $e_1$ to $f$,
then evaluate $e_2$ to $x$, and then evaluate the application ``$f~x$'', all the while keeping
track of any side effects that are produced.

If our source language was actually effectful, for example with built-in effects
``$\textsf{\color{ACMDarkBlue}get}$'' and ``$\textsf{\color{ACMDarkBlue}put}$'', we could fix
$\textsf{M} = \textsf{State}$ and add translations
\begin{align*}
  \textsf{\color{ACMDarkBlue}get}^* &= [x \mid x \leftarrow \textsf{get}]^\textsf{State} \\
  (\textsf{\color{ACMDarkBlue}put}~e)^* &= [u \mid x \leftarrow e^*;\ u \leftarrow \textsf{put}~x]^\textsf{State}
\end{align*}
where \textsf{get} and \textsf{put} are defined as in Section \ref{sec:monad-examples}. Given a
once-and-for-all proof that the monadic version has the same semantics as the impure version,
analysis could be done on the pure language instead. In addition, this kind of translation might
streamline the implementation of a compiler or interpreter.

% Wadler also gives a call-by-name version of the translation, which lifts a program of type
% \begin{center}
% $\alpha \to \beta$ \hspace{5mm} to one of type \hspace{5mm} $\textsf{M}~\alpha \to \textsf{M}~\beta$.
% \end{center}
% In call-by-name, the function arguments are computations as well. The translation is written
% $e^\dagger$:
% \begin{align*}
%   x^\dagger &= x \\
%   (\lambdaE{x}{e})^\dagger &= [\lambdaE{x}{e^\dagger}]^\textsf{M} \\
%   (e_1~e_2)^\dagger &= [y \mid f \leftarrow e_1^\dagger;\ y \leftarrow f~e_2^\dagger]^\textsf{M} \\
%   (e_1,\, e_2)^\dagger &= [(e_1^\dagger,\, e_2^\dagger)]^\textsf{M} \\
%   (\textsf{\color{ACMDarkBlue}fst}~e)^\dagger &= [y \mid x \leftarrow e^\dagger;\ y \leftarrow \textsf{\color{ACMDarkBlue}fst}~x]^\textsf{M}
% \end{align*}
% This is a more obscure translation, but it is useful when the source language is call-by-name.

\outline{redo} These translations mark the last bit of background that we need; now we can start to explore the
connections between monads and delimited continuations.

\section{Common Design Patterns} \label{sec:patterns}
I said in Section \ref{sec:introduction} that both delimited continuations and monads abstract
away complex control flow and simulate effectful code in a pure language. So far we have
certainly seen that monads can do this, but it is less clear that delimited continuations can do
the same. In particular, we have only seen delimited continuations simulate exceptions and
nondeterminism. In this section I will complete the picture and a few more abstractions \outline{ew} that can
be captured using continuations. I was inspired by a blog post by \citet{xia_2019}, which takes
all of these ideas even further, implementing various monads embedded in the {\em continuation
monad} in Haskell---I will discuss that monad in detail in Section \ref{sec:contmonad}.

Following Xia's presentation, I will present each pattern by defining (1) the primitive
operations that are core to the abstraction and (2) a \textsf{run} function that delimits the
scope of the computation.

\subsubsection{Maybe}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{abort}$ & $=$ & $\shift{k}{\textsf{Nothing}}$ \\
    $\textsf{run}^{\textsf{Maybe}}~c$ & $=$ & $\reset{\textsf{Just}~c}$
  \end{tabular}
\end{center}
We have seen exceptions in the contexts of both continuations and monads already, although
admittedly I have been fairly imprecise when talking about using shift to throw an exception.
This construction faithfully simulates the \textsf{Maybe} monad, in that its computations either
fail and return $\nothing$ or succeed with a value $v$ and return $\just{v}$.

\subsubsection{Identity} \outline{can we do strictness?}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{noop}~x$ & $=$ & $\shift{k}{k~x}$ \\
    $\textsf{run}^{\textsf{Id}}~c$ & $=$ & $\reset{c}$
  \end{tabular}
\end{center}
Identity can be encoded without using continuations at all, of course, but this presentation is
instructive. Here the \textsf{noop} operation shows how to do nothing using delimited
continuations---just call the continuation. All \textsf{run} needs to do is delimit the
continuation scope.

\subsubsection{State}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\shift{k}{\lambdaE{s}{k~s~s}}$ \\
    $\textsf{put}~s'$ & $=$ & $\shift{k}{\lambdaE{s}{k~()~s'}}$ \\
    $\textsf{run}^{\textsf{State}}~c~i$ & $=$ & $\reset{(\lambdaE{v}{\lambdaE{s}{v}})~c}~i$
  \end{tabular}
\end{center}
This example is considerably more involved. As with the state monad, encoding state with
continuations is about capturing state in function parameters and return values. Our
continuations take two parameters: the first is the computation value, and the second is the new
state. The \textsf{get} operation takes in the current state and continues with that state as
both the computation value and the new state. The \textsf{put} operation ignores the current
state and continues with unit as the computation value and $s'$ as the new state. The
\textsf{run} function takes an initial state and passes it in {\em outside} the continuation
scope.

As with the monadic version, these operations can be used to simulate stateful code. For example,
this program
\begin{align*}
\textsf{run}^{\textsf{State}}~(& \textsf{\color{ACMDarkBlue}let}~i = \textsf{get}~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{put}~(i + 3); \\
& \textsf{\color{ACMDarkBlue}let}~j = \textsf{get}~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{put}~(j * 7); \\
& \textsf{get})~3
\end{align*}
returns 42 as one might expect, and does so without references or other built-in state constructs.

\subsubsection{Reader}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\textsf{get}$ \\
    $\textsf{run}^{\textsf{Reader}}$ & $=$ & $\textsf{run}^{\textsf{State}}$
  \end{tabular}
\end{center}
\textsf{Reader} is strictly simpler than \textsf{State}, but we can use the same definitions
anyway. We just rename \textsf{get} to \textsf{ask} and don't provide a \textsf{put} operation.

\subsubsection{Nondeterminism}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\shift{k}{k~\varnothing}$ \\
    $\textsf{flip}$ & $=$ & $\shift{k}{k~\textsf{\color{ACMDarkBlue} true} \cup k~\textsf{\color{ACMDarkBlue} false}}$ \\
    $\textsf{run}^{\textsf{ND}}~c$ & $=$ & $\reset{(\lambdaE{v}{\{v\}})~c}$
  \end{tabular}
\end{center}
We already saw one way of encoding nondeterminism using continuations (see Section
\ref{sec:danvy:nondet}), but here is another one that is a bit closer to the monadic
presentation. We again make use of sets to define the primitive operations $\textsf{fail}$ and
$\textsf{flip}$: Our implementation of \textsf{run} lifts computations into singleton sets, and
our operators call the continuation on whatever values should be considered---either nothing in
the case of \textsf{fail} or both $\textsf{\color{ACMDarkBlue} true}$ and
$\textsf{\color{ACMDarkBlue} false}$ in the case of $\textsf{flip}$.

\subsection{Are there more?}
This is just the tip of the iceberg when it comes to monadic design patterns that can be
implemented using delimited continuations---in fact, a few years after {\em Abstracting Control},
\citet{filinski1994representing} published another paper showing that delimited continuations can
represent all ``pure'' monads (i.e., those whose \textsf{map}, \textsf{join}, and \textsf{bind}
operations can be implemented in a pure language). We discuss this result further in Section
\ref{sec:conclusion}.

\section{The Continuation Monad} \label{sec:contmonad}
Up to this point we have viewed delimited continuations and monads in parallel, bouncing back and
forth between the abstractions and showing how they compare. But there is one final insight from
Wadler's paper that I skipped over, and I will use it now to build a bridge between continuations
and monads.

Towards the end of {\em Comprehending Monads}, Wadler presents the {\em continuation monad}.
Given a result type, $\rho$, the continuation monad is defined as:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Cont}$~$\alpha$ & $=$ & $(\alpha \to \rho) \to \rho$ \\
    $\map{Cont}{f}{\overline{x}}$ & $=$ & $\lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\kappa~(f~x)})}$ \\
    $\unit{Cont}{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~x}$ \\
    $\join{Cont}{\doverline{x}}$ & $=$ &
      $\lambdaE{\kappa}{\doverline{x}~(\lambdaE{\overline{x}}{\overline{x}~\kappa})}$
  \end{tabular}
\end{center}
Intuitively, $\textsf{Cont}~\alpha$ is the type of computations that take a continuation
expecting an input $\alpha$ and computing a result in a fixed type $\rho$. The \textsf{map}
operation composes a function $f$ with the continuation, \textsf{unit} simply continues with a
given value, and \textsf{join} untangles nested continuations to enable chained computations.

Programs written using the continuation monad are automatically in a continuation-passing style.
A sequence of operations
\[ [(x, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{Cont}} \]
expands to
\[ \lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\overline{y}~(\lambdaE{y}{\kappa~(x,\, y)})})}, \]
which is precisely the way a tuple of expressions would be evaluated in CPS.

As with many of the monads before, we can define auxiliary operations that interact in useful ways
with the monad operations. We can define our old friends \textsf{shift} and \textsf{reset}!
\begin{center}
  \begin{tabular}{lll}
    $\textsf{shift}$~$f$ & $=$ & $\lambdaE{\kappa}{f~(\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}})~(\lambdaE{x}{x})}$ \\
    $\textsf{reset}$~$\overline{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~(\overline{x}~(\lambdaE{x}{x}))}$
  \end{tabular}
\end{center}
Whereas in Section \ref{sec:danvy} we built these operations into the language, now they are
simply defined. This means that there is no need to extend any meta-theory---we can interpret these
operations in the same lambda calculus that we worked with in Section \ref{sec:wadler}.

In particular, this means that we can use the \textsf{Cont} monad comprehension to write programs like
\[
  [x + y \mid x \leftarrow [1];\ y \leftarrow \textsf{reset}~[u + v \mid u \leftarrow [10];\ v \leftarrow \textsf{shift}(\lambdaE{k}{[b \mid a \leftarrow k~100;\ b \leftarrow k~a]})]]. \\
\]
which is equivalent to the program
\[ 1 + \reset{10 + \shift{k}{k~(k~100)}}, \]
which we saw when discussing {\em Abstracting Control} (although admittedly the monad version is
a bit more verbose). Critically neither version relies on manually managing continuation
parameters; they are both written in a ``direct'' style.

\subsection{Recovering ECPS}
We can tie everything together by looking back at Wadler's call-by-value translation into a monad
\textsf{M}. Recall that the translation takes a function of type $\alpha \to \beta$ in a language
with true effectful operators, and produces a program of type $\alpha \to \textsf{M}~\beta$ in a
pure language where \textsf{M} captures the effect. Though we have not discussed it this way so
far, shift and reset are actually built-in effects (for some definition of effect) \outline{not
great} so we can use the $(\cdot)^*$ transformation to capture those effects in the \textsf{Cont}
monad.

Concretely, we will use $(\cdot)^*$ to translate programs from Section \ref{sec:danvy} into
programs from Section \ref{sec:wadler} where $\textsf{M} = \textsf{Cont}$. By extending Wadler's
$(\cdot)^*$ translation with rules that turn ``$\shift{k}{e}$'' into
``$\textsf{shift}~(\lambdaE{k}{e})$'' and ``$\reset{e}$'' into ``$\textsf{reset}~e$'', we obtain
exactly the ECPS translation presented by Danvy and Filinski!
\begin{align*}
  x^* &= [x]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~x} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{e^*})} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{f}{e_2^*~(\lambdaE{x}{f~x~\kappa})})} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{x}{e_2^*~(\lambdaE{y}{\kappa (x,\, y)})})} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{Cont} &= \lambdaE{\kappa}{e^*~(\lambdaE{x}{\textsf{\color{ACMDarkBlue} fst}~x)}} \\
  (\shift{k}{e})^* &= \textsf{shift}~(\lambdaE{k}{e^*}) &= (\lambdaE{\kappa}{e^*~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \reset{e}^* &= \textsf{reset}~e^* &= \lambdaE{\kappa}{\kappa~(e^*~\textsf{id})}
\end{align*}

This is truly beautiful. It means that the continuation monad entirely captures the semantics of
a language with first-class continuation operators. Shockingly, even though Danvy and Filinski
gave their language no fewer than four semantic interpretations (including ECPS, a denotational
semantics, and two meta-circular interpreters) Wadler managed to find one more way of looking at
continuation semantics.

(Of course, Wadler did not present exactly this result. He observed that the call-by-name
translation results in the unusual call-by-name version of the CPS translation, but since shift
and reset did not exist, he did not complete the ECPS picture. I am sure that the full ECPS--monad
embedding is not novel, but I completed the picture independently.)

\section{Conclusion} \label{sec:conclusion}
\outline{Start to wrap up...}

\citet{filinski1994representing} provides a detailed account of how monadic computations can be
represented using delimited continuations. While my constructions in Section \ref{sec:patterns}
are ad-hoc and dependent on the particular monad in question, Filinski managed to give a
translation that represents monadic patterns with continuations once-and-for-all. Still, the
ad-hoc connections are valuable: they provide more granular intuition than Filinski's
presentation, and they are all entirely pure, while Filinski's formulation relies on a reference
cell.

In recent years there has been increasing focus on {\em algebraic effects}.\outline{cite}
Interestingly, algebraic effects have links to both continuations and monads: they operate via
{\em handlers}, which are often written using explicit continuations, but they isolate effects in
much the same way as monadic abstractions.

\outline{Better ending}

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}

% \begin{quote}
%   [In some languages,] it may be possible to jump out of an expression and then later jump back
%   into it again and resume the process of evaluation. Continuations are sufficiently powerful to
%   deal with such a situation. (This could not be taken to imply approval of jumps back into
%   expressions as a language design feature—but if a language can specify something, however odd,
%   the method used to give its formal semantics must be powerful enough to describe it.)
%   \cite{strachey2000continuations}
% \end{quote}
