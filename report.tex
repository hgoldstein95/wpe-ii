\documentclass[acmsmall, nonacm, screen]{acmart}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{xcolor}
\usepackage{stmaryrd}
\usepackage{anyfontsize}

\newif\ifdraft\drafttrue

\definecolor{green}{HTML}{298a33}
\definecolor{orange}{HTML}{995d02}

\newcommand{\outline}[1]{
  \ifdraft
  {\color{red}{#1}}
  \fi
}

\newcommand\doverline[1]{%
  \setbox0=\hbox{$\overline{#1}$}%
  \ht0=\dimexpr\ht0-.30ex\relax% CHANGE .15 TO AFFECT SPACING
  \overline{\copy0}%
}

\newcommand{\ifThenElse}[3]{\textsf{\color{ACMDarkBlue}if}~#1~\textsf{\color{ACMDarkBlue}then}~#2~\textsf{\color{ACMDarkBlue}else}~#3}
\newcommand{\caseOf}[2]{\textsf{\color{ACMDarkBlue} case}~#1~\textsf{\color{ACMDarkBlue}of}~\{~#2~\}}
\newcommand{\letIn}[3]{\textsf{\color{ACMDarkBlue}let}~#1 = #2~\textsf{\color{ACMDarkBlue}in}~#3}
\newcommand{\shift}[2]{\textsf{\color{ACMDarkBlue}shift}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\callcc}[2]{\textsf{\color{ACMDarkBlue}call/cc}~#1~\textsf{\color{ACMDarkBlue}in}~#2}
\newcommand{\reset}[1]{\langle #1 \rangle}
\newcommand{\lambdaE}[2]{\lambda #1.\, #2}
\newcommand{\just}[1]{\textsf{Just}~#1}
\newcommand{\nothing}{\textsf{Nothing}}
\newcommand{\map}[3]{\textsf{map}^{\textsf{#1}}~#2~#3}
\newcommand{\unit}[2]{\textsf{unit}^{\textsf{#1}}~#2}
\newcommand{\join}[2]{\textsf{join}^{\textsf{#1}}~#2}
\newcommand{\cps}[1]{\mathcal{C}\llbracket #1 \rrbracket}
\newcommand{\cpsm}[1]{\mathcal{C}'\llbracket #1 \rrbracket}
\newcommand{\cpsmc}[1]{\mathcal{C}''\llbracket #1 \rrbracket}
\newcommand{\denote}[1]{\mathcal{E}\llbracket #1 \rrbracket}
\newcommand{\stringE}[1]{\textsf{\color{green} ``#1''}}
\newcommand{\quoteE}[1]{{\color{orange} \ulcorner #1 \urcorner}}
\newcommand{\unquoteE}[1]{{\color{black} \llparenthesis #1 \rrparenthesis }}

\lstset{ %
  backgroundcolor=\color{white},
  commentstyle=\color{ACMGreen},
  keywordstyle=\color{ACMDarkBlue},
  stringstyle=\color{ACMPurple},
  basicstyle=\ttfamily
}

\lstdefinestyle{hs}{
  language=Haskell
}

\lstdefinestyle{rkt}{
  language=lisp,
  deletekeywords={get},
  morekeywords={define},
  literate=*{(}{{\textcolor{gray}{(}}}{1}
    {)}{{\textcolor{gray}{)}}}{1}
}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\acmBooktitle{}

\begin{document}

\title{Delimited Continuations and Monads}
\subtitle{A Comparison of Programming Abstractions}
\titlenote{
  This report was compiled for part of the University of Pennsylvania's WPE-II Exam. The
  accompanying talk is available on the author's website.
}

\author{Harrison Goldstein}
\email{hgo@seas.upenn.edu}
\orcid{0000−0001−9631−1169}
\affiliation{%
  \institution{University of Pennsylvania}
  \city{Philadelphia, PA}
  \country{USA}
}

\renewcommand{\shortauthors}{Goldstein}

\begin{abstract}
  In 1990, two programming abstractions were introduced independently: {\em delimited
  continuations} and {\em monads}. \citet{danvy1990abstracting} explored delimited continuations
  as a principled way of manipulating program contexts as first-class functions.
  \citet{wadler1990comprehending} popularized monads as a tool for simulating effects in a pure
  language. Though they initially seem to have little to do with one-another, delimited
  continuations and monads actually have a lot in common: they can implement many of the same
  design patterns, and their meta-theories are surprisingly compatible. This report
  re-contextualizes those early 90's papers and explores the commonalities between monads and
  delimited continuations.
\end{abstract}

\maketitle

\section{Introduction} \label{sec:introduction}
When writing an algorithm, programmers want to focus on the interesting parts and ignore the
implementation details. Error cases can be neatly packaged up in exceptions, mutable state can be
implemented with references, and non-deterministic choices can be made by random number
generators, all while the programmer focuses on the important parts of their algorithm.
Unfortunately, languages with built-in {\em effects} like errors, state, and nondeterminism are
often harder to reason about than computationally {\em pure} languages without such features. Is
it possible abstract away effects {\em and} keep the benefits of purity? Two abstractions, developed
independently in the early 1990's, provide ways to do just that.

Consider the following programs that perform simple error handling:
\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=rkt, deletekeywords={abort}, mathescape]
  (define (p n)
    (let ([i 84]
          [j (if (= n 0)
               (abort)
               n)])
      (/ i j)))

  (run ($\lambda$ () (p 2)) ; '('Just 42)
  (run ($\lambda$ () (p 0)) ; 'Nothing
  \end{lstlisting}
  \label{fig:racket-state}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \begin{lstlisting}[style=hs]
  p n = do
    i <- return 84
    j <- if n == 0
      then abort
      else return n
    return (i `div` j)

  run (p 2) -- Just 42
  run (p 0) -- Nothing
  \end{lstlisting}
  \label{fig:hs-state}
\end{subfigure}
\Description[State Abstraction]{Code for manipulating state.}
\label{fig:stateful-comps}
\end{figure}
\noindent Both of these programs are pure---they do not use exceptions or other built-in effects.
The error handling is done entirely by underlying abstractions that allow the programmer to think
in terms of exceptions without adding native effects to the language.

The first snippet, written in Racket, uses {\em delimited continuations} to implement error
handling. A continuation is a first-class representation of the "rest" of a computation. For
example, when evaluating the sub-expression ``$2$'' in a simplified version of our above example,
\[ \letIn{i}{84}{\letIn{j}{2}{i / j}}, \]
the remaining {\em context} is
\[ \letIn{i}{84}{\letIn{j}{\_}{i / j}}, \]
and the continuation, $k$, is
\[ k = \lambdaE{x}{\letIn{i}{84}{\letIn{j}{x}{i / j}}}. \]
The continuation captures the context as a function. In {\em Abstracting Control},
\citet{danvy1990abstracting} present two operators for manipulating continuations: {\em shift}
(written ``$\shift{k}{e}$'') and {\em reset} (written ``$\reset{e}$''). Reset delimits the scope
of the current context---in the example above, we implicitly wrap the whole expression in a
reset. The expression ``$\shift{k}{e}$''captures the current context as a continuation, $k$, and
then executes $e$ in a fresh context. We can use shift and reset to extract the continuation from
above:
\begin{align*}
  & \reset{\letIn{i}{84}{\letIn{j}{(\shift{k}{k~2})}{i / j}}} \\
  \rightarrow\ & \letIn{k}{(\lambdaE{x}{\letIn{i}{84}{\letIn{j}{x}{i / j}}})}{k~2}
\end{align*}
Reset delimits the context, in this case the whole expression, and shift packages the
continuation into $k$. In this example the shift body just applies $k$, so nothing has
changed---the result will be the same as the original expression. But the shift body can be
anything. In particular, if $k$ is not used at all
\[ \reset{\letIn{i}{84}{\letIn{j}{(\shift{k}{\textsf{Nothing}})}{i / j}}} \]
the result is essentially an {\em exception}: the expression is thrown away and the result of the
computation is just \textsf{Nothing}. This is roughly how the Racket example above is
implemented.

The second snippet simulates error-handling computations using a {\em monad} in Haskell. Monads
were introduced to the programming languages literature by \citet{moggi1991notions}, but they
were popularized by \citeauthor{wadler1990comprehending} in {\em Comprehending
Monads}~\cite{wadler1990comprehending}. As a programming abstraction, monads generalize the
well-known idea of list comprehensions:
\[
  [ (x,\,y) \mid x \leftarrow [1,\, 2] ;\ y \leftarrow [3,\, 4] ]^{\textsf{List}}
\]
This expression results in the Cartesian product of lists ``$[1,\, 2]$'' and ``$[3,\, 4]$.''
Wadler observed that comprehension syntax can be used to program with any monad (not just
$\textsf{List}$):
\[
  [ i / j \mid i \leftarrow \just{84} ;\ j \leftarrow \ifThenElse{n = 0}{\textsf{Nothing}}{\just{n}} ]^{\textsf{Maybe}}
\]
This monad, usually called $\textsf{Maybe}$, is another way to implement pure exceptions: if the
result of comprehension will be ``$\nothing$'' if $n = 0$, otherwise the result will be
``$\just{42}$.'' There are dozens of monads that programmers use in practice, each of which gives
a different interpretation of comprehension syntax.

It is not immediately clear that manipulating continuations has anything to do with interpreting
comprehension syntax. One would be forgiven for assuming that these ideas are largely orthogonal.
But we have already seen that that delimited continuations and monads can both be used to
simulate exceptions. Could there be more overlap? Yes! It turns out that that all of the effects
we discussed before, and a number of others, can be implemented using either delimited
continuations or monads. Furthermore, continuations actually form a monad, and
\citeauthor{wadler1990comprehending} shows that this fact can be exploited to to recover results
from \citeauthor{danvy1989functional}.

In this report I make three contributions:
\begin{itemize}
  \item I summarize and re-contextualize two influential papers: {\em Abstracting Control} by
  \citeauthor{danvy1990abstracting} (\S~\ref{sec:danvy}) and {\em Comprehending Monads} by
  \citeauthor{wadler1990comprehending} (\S~\ref{sec:wadler}).\footnote{This is a requirement of
  the WPE-II exam, but I hope that reformulating these results will still be instructive.}
  \item I show the many design patterns that can be implemented equally well by both delimited
  continuations and monads, establishing a close relationship between the two language features
  (\S~\ref{sec:patterns}).
  \item I explore the {\em continuation monad} and show that by performing a monad-agnostic
  transformation given by \citeauthor{wadler1990comprehending} we can recover the ``extended
  continuation passing style'' transformation presented by \citeauthor{danvy1989functional}
  (\S~\ref{sec:contmonad}). This is a small extension of Wadler's original result.
\end{itemize}
I conclude with some remarks on the broader context in which these abstractions exist
(\S~\ref{sec:conclusion}).

\subsection{Unified Notation} \label{sec:notation}
My two source papers differ slightly on notations and conventions. To aid with comprehension, I
have chosen a reasonable middle-ground and use one unified set of conventions to discuss both
papers. Throughout this report, assume that programs are written in a lambda calculus given by
the grammar
\begin{align*}
  e \Coloneqq &\ x \mid \lambdaE{x}{e} \mid e_1~e_2 \\
             | &\ \textsf{\color{ACMDarkBlue} true} \mid \textsf{\color{ACMDarkBlue} false} \mid \ifThenElse{e_1}{e_2}{e_3} \\
             | &\ (e_1,\, e_2) \mid \textsf{\color{ACMDarkBlue} fst}~e \mid \textsf{\color{ACMDarkBlue} snd}~e.
\end{align*}
Unless otherwise specified, the calculus is call-by-value. Also, since we are almost exclusively
concerned with the dynamic semantics of these programs, I will not formalize a type system.
Occasionally I will ignore either Booleans or pairs for brevity.

\section{{\em Abstracting Control}} \label{sec:danvy}
Before explaining Danvy and Filinski's contributions, I should explain some background on
continuations and first-class continuation operators. Continuations first appeared as
meta-theoretic tools \outline{cite}, but before long they were introduced into concrete languages
via the {\em continuation-passing style} or CPS translation. Here is the translation,
$\cps{\cdot}$, for our basic lambda calculus with Booleans and if-statements:
\begin{align*}
  \cps{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cps{\lambdaE{x}{e}} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{\cps{e}})} \\
  \cps{e_1~e_2} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \\
  \cps{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cps{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{b}{\ifThenElse{b}{\cps{e_2}~\kappa}{\cps{e_3}~\kappa}})}
\end{align*}
Translated terms do not ``return'' in the traditional sense. When these terms finish computing,
they pass the resulting value to a continuation, $\kappa$, which is provided as an argument. 
A translated term, can be run by passing the trivial continuation: $\cps{e}~(\lambdaE{x}{x})$.

The CPS translation has been used extensively in compilers and interpreters, where it gives
fine-grained control over evaluation. This is because the translation makes evaluation order
explicit---this particular CPS translation is clearly call-by-value, since in the rule for
application,
\[ \cps{e_1~e_2} = \lambdaE{\kappa}{\cps{e_1}~(\lambdaE{f}{\cps{e_2}~(\lambdaE{x}{f~x~\kappa})})} \]
the argument $e_2$ is evaluated before $f$ is applied.

Once the CPS translation became established researchers began to consider first-class
abstractions for working with continuations. The most famous of these early abstractions, ``call
with current continuation'' (or ``call/cc'') \outline{cite}, can be added as an operator in our
source language and implemented as an extension to the CPS translation:
\[ \cps{\callcc{k}{e}} = (\lambdaE{\kappa}{\cps{e}~\kappa})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa~x}}], \]
where ``$e[x \mapsto v]$'' means $e$ with $v$ substituted for free instances of $x$. The call/cc
operator is extremely powerful. The expression $\callcc{k}{e}$ captures the current program
context as $k$ and then runs $e$, allowing the programmer to arbitrarily pause and resume
evaluation as they see fit. For example,
\[ \callcc{k}{1} \]
throws an exception with the value $1$ and
\[ \callcc{k}{(\textsf{\color{ACMDarkBlue} print}~\stringE{foo};\ k~10)} \]
pauses the computation, prints ``foo'', and then resumes computation with the value 10.
Unfortunately, many have said that call/cc is {\em too} powerful. Consider the popular Yin-Yang
Puzzle:
\begin{align*}
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yin}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{@};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{\color{ACMDarkBlue} let}\ \textsf{yang}\ =\ (\lambdaE{c}{(\textsf{\color{ACMDarkBlue} print}~\stringE{*};\ c)})~(\callcc{k}{k})~\textsf{\color{ACMDarkBlue} in} \\
& \textsf{yin}~\textsf{yang}
\end{align*}
What does this confusing mess of continuations do? Apparently it counts. This program prints
increasing numbers in a unary representation,
\[ \stringE{@*@**@***@****@*****...}, \]
but it is extremely difficult to understand exactly why that is.

\citet{felleisen1988theory} attempted to reign in the power of call/cc with operators that he
called ``prompt'' and ``control''. These allowed programmers to delimit the effects of call/cc
with {\em scopes}. Unfortunately, Felleisen's scopes were dynamic, and his approach did not admit
a straightforward translation into a standard lambda calculus. Danvy and Filinski built on
Felleisen's approach and introduced statically delimited continuations.

\subsection{Delimited Continuations}
In {\em Abstracting Control}, Danvy and Filinski introduce the ``shift'' and ``reset'' operations
mentioned in Section \ref{sec:introduction}, which provide a more usable alternative to call/cc.
\[
  e \Coloneqq \cdots \mid \shift{k}{e} \mid \reset{e}
\]
(Note that Danvy and Filinski occasionally wrote $\xi k.\, e$ instead of $\shift{k}{e}$.) These
operators can also be interpreted via a modified CPS translation that Danvy and Filinski call
{\em extended continuation-passing style} (ECPS):
\begin{align*}
  \cps{\shift{k}{e}} &= (\lambdaE{\kappa}{\cps{e}~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \cps{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cps{e}~\textsf{id})}
\end{align*}
Shift and reset provide a more intuitive way of working with continuations. Reset clearly
delimits the bounds of the current context, and shift gives programmatic access to that context.
For example, in the expression
\begin{align*}
& 1 + \reset{10 + \shift{k}{k~(k~100)}} \Rightarrow \\
& 1 + (10 + (10 + 100)) \Rightarrow \\
& 121,
\end{align*}
the continuation ``$k = \lambdaE{x}{10 + x}$'' is applied twice to $100$ before finishing the
evaluation by adding $1$, resulting in $121$.

In addition to the ECPS translation, Danvy and Filinski give a denotational semantics for the
lambda calculus extended with shift and reset. Assuming $\textsf{Ans}$ is a suitable domain of
final answers, we define
\begin{center}
  \begin{tabular}{llr}
    $\rho \in \textsf{Env}$ & $=$ & $\textsf{Var} \rightharpoonup \textsf{Val}$ \\
    $\gamma \in \textsf{MCont}$ & $=$ & $\textsf{Val} \to \textsf{Ans}$ \\
    $\kappa \in \textsf{Cont}$ & $=$ & $\textsf{Val} \to \textsf{MCont} \to \textsf{Ans}$ \\
    $\mathcal{E}$ & $:$ & $\textsf{Exp} \to \textsf{Env} \to \textsf{Cont} \to \textsf{MCont} \to \textsf{Ans}$.
  \end{tabular}
\end{center}
Intuitively $\rho$ is the variable environment, $\gamma$ is the meta-continuation, and $\kappa$
is the continuation.
The denotational semantics is given by the equations
\begin{align*}
  \denote{x}~\rho~\kappa~\gamma &= \kappa~(\rho[x])~\gamma \\
  \denote{\lambdaE{x}{e}}~\rho~\kappa~\gamma &= \kappa~(\lambdaE{v}{\denote{e}~(\rho[x \mapsto v])})~\gamma \\
  \denote{e_1~e_2}~\rho~\kappa~\gamma &=
    \denote{e_1}~\rho~(\lambdaE{f}{\denote{e_2}~\rho~(\lambdaE{x}{f~x~\kappa})})~\gamma \\
  \denote{\textsf{\color{ACMDarkBlue}true}}~\rho~\kappa~\gamma &= \kappa~\textsf{\color{ACMDarkBlue} true}~\gamma \\
  \denote{\ifThenElse{e_1}{e_2}{e_3}}~\rho~\kappa~\gamma &= 
    \denote{e_1}~\rho~(\lambdaE{b}{\ifThenElse{b}{\denote{e_2}~\rho~\kappa}{\denote{e_3}~\rho~\kappa}})~\gamma \\
  \denote{\shift{k}{e}}~\rho~\kappa~\gamma &=
    \denote{e}~(\rho[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\lambdaE{\gamma'}{\kappa~x~(\lambdaE{w}{\kappa'~w~\gamma'})}}}])~(\lambdaE{x}{\lambdaE{\gamma''}{\gamma''~x}})~\gamma \\
  \denote{\reset{e}}~\rho~\kappa~\gamma &= \denote{e}~\rho~(\lambdaE{x}{\lambdaE{\gamma'}{\gamma'~x}})~(\lambdaE{x}{\kappa~x~\gamma})
\end{align*}
where constructions on the right-hand side are understood as syntax of a meta-language, rather
than as concrete syntax. Note that some $\gamma$ arguments are $\eta$-reduced away to simplify
presentation---in fact, they can be entirely elided for all rules other than the ones for shift
and reset.

\outline{Don't like this paragraph} This denotational semantics is quite similar in form to the
ECPS translation but the meta-continuation $\gamma$ gives a bit more intuition about what is
going on with shift and reset that is different from call/cc.
\[ 
  \denote{\callcc{k}{e}}~\rho~\kappa~\gamma =
    \denote{e}~(\rho[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa~x}}])~\kappa~\gamma \\
\]
Shift and reset use the meta-continuation to keep track of the continuation scope, which makes it
easier to avoid the confusing ``action at a distance'' that call/cc sometimes creates.

\subsection{Metacircular Interpreters}
A {\em metacircular interpreter}, introduced by \citet{reynolds1972definitional}, is a powerful
tool for expressing a language's semantics. Rather than translate code into abstract objects or
describe it via operational rules, a metacircular interpreter defines a language by translating
its constructs into the equivalent constructs in a well-understood meta-language. Danvy and
Filinski use this approach to build a more efficient ECPS translation.

For this interpreter, our meta-language is the same lambda calculus we have been working with,
but without shift or reset and with a very simple {\em quoting} mechanism. In the following
program, the code in black is the interpreter and it produces $\quoteE{\text{quoted}}$ code as an
output (we use $\unquoteE{\text{unquote brackets}}$ to splice computations into quoted segments).
\begin{align*}
  \cpsm{x} &= \lambdaE{\kappa}{\kappa~x} \\
  \cpsm{\lambdaE{x}{e}} &=
    \lambdaE{\kappa}{\kappa~\quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{k}{\unquoteE{\cpsm{e}~(\lambdaE{a}{\quoteE{k~\unquoteE{a}}})}}}}} \\
  \cpsm{e_1~e_2} &= \lambdaE{\kappa}{\cpsm{e_1}~(\lambdaE{f}{\cpsm{e_2}~(\lambdaE{x}{\quoteE{\unquoteE{f}~\unquoteE{x}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}})})} \\
  \cpsm{\textsf{\color{ACMDarkBlue}true}} &= \lambdaE{\kappa}{\kappa~\textsf{\color{ACMDarkBlue} true}} \\
  \cpsm{\ifThenElse{e_1}{e_2}{e_3}} &= \lambdaE{\kappa}{\cpsm{e_1}~(\quoteE{\lambdaE{b}{\ifThenElse{b}{\unquoteE{\cpsm{e_2}~\kappa}}{\unquoteE{\cpsm{e_3}~\kappa}}}})} \\
  \cpsm{\shift{k}{e}} &= (\lambdaE{\kappa}{\cpsm{e}~\textsf{id}})[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}] \\
  \cpsm{\reset{e}} &= \lambdaE{\kappa}{\kappa~(\cpsm{e}~\textsf{id})}
\end{align*}
The final ECPS result is given by ``$\cpsm{e}~\textsf{id}$''. This optimized translation avoids
inserting unnecessary redexes, and produces a much simpler ECPS result then the original
translation.

It turns out that we can do even better if we include delimited continuation operators in our
meta-language. This final ECPS translation, 
\begin{align*}
  \cpsmc{x} &= x \\
  \cpsmc{\lambdaE{x}{e}} &= \quoteE{\lambdaE{\unquoteE{x}}{\lambdaE{\kappa}{\unquoteE{\reset{\quoteE{\kappa~\unquoteE{\cpsmc{e}}}}}}}} \\
  \cpsmc{e_1~e_2} &= \shift{\kappa}{\quoteE{\unquoteE{\cpsmc{e_1}}~\unquoteE{\cpsmc{e_2}}~(\lambdaE{t}{\unquoteE{\kappa~\quoteE{t}}})}} \\
  \cpsmc{\textsf{\color{ACMDarkBlue}true}} &= \textsf{\color{ACMDarkBlue}true} \\
  \cpsmc{\ifThenElse{e_1}{e_2}{e_3}} &= \shift{\kappa}{\quoteE{\ifThenElse{\unquoteE{\cpsmc{e_1}}}{\unquoteE{\reset{\kappa~\cpsmc{e_2}}}}{\unquoteE{\reset{\kappa~\cpsmc{e_3}}}}}} \\
  \cpsmc{\shift{k}{e}} &= \shift{\kappa}{\reset{\cpsmc{e}}[k \mapsto \quoteE{\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\unquoteE{\kappa~\quoteE{x}}}}}]} \\
  \cpsmc{\reset{e}} &= \reset{\cpsmc{e}}
\end{align*}
also avoids unnecessary $\eta$-expansion, resulting in an extremely compact final representation.

This final interpreter is essentially written in the same language that it interprets---this is
why metacircular interpreters are called ``meta'' and it does pose a bootstrapping problem. I
said above that the meta-language should already be well-understood. Luckily, we already have
both the ECPS translation and our denotational semantics as descriptions of this meta-language
(modulo quoting, which is simple). The efficient interpreter can be written in this meta-language
and then that interpreter can itself be ECPS transformed into a standard lambda calculus.

\subsection{Use Case: Nondeterministic Programming} \label{sec:danvy:nondet}

Delimited control operators can be used to solve complex programming problems. Danvy and Filinski
choose to highlight one application in particular: nondeterministic programming. The nondeterministic
operators \textsf{fail} and \textsf{flip} are defined first:
\begin{align*}
\textsf{fail}~() &= \shift{k}{\stringE{failure}} \\
\textsf{flip}~() &= \shift{k}{(k~\textsf{\color{ACMDarkBlue}true};\ k~\textsf{\color{ACMDarkBlue}false};\ \textsf{fail}~())}
\end{align*}
The \textsf{fail} operation just acts as an exception, throwing away the continuation and
returning the string $\stringE{failure}$. The main source of nondeterminism is \textsf{flip}---it
actually calls its continuation on {\em both} \textsf{\color{ACMDarkBlue}true} and
\textsf{\color{ACMDarkBlue}false}, but the caller is expected to use \textsf{flip} as if it
nondeterministically chooses between the two options (in model of nondeterminism, those two
points of view coincide). Finally, \textsf{choice} builds on \textsf{flip} to simulate a
nondeterministic choice of integers less than a given value.
\[
  \textsf{choice}~n = \ifThenElse{n < 1}{\textsf{fail~()}}{\ifThenElse{\textsf{flip}~()}{\textsf{choice}~(n - 1)}{n}}
\]

The provided operators can be used to implement
\begin{align*}
\textsf{triple}~n~s =&  \\
& \textsf{\color{ACMDarkBlue}let}~i = \textsf{choice}~n~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~j = \textsf{choice}~(i - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{\color{ACMDarkBlue}let}~k = \textsf{choice}~(j - 1)~\textsf{\color{ACMDarkBlue}in} \\
& \ifThenElse{i + j + k = s}{(i,\, j,\, k)}{\textsf{fail}~()},
\end{align*}
which finds all triples of distinct positive integers that sum to a given integer $s$. Evaluating
the expression ``$\reset{\textsf{\color{ACMDarkBlue}print}~(\textsf{triple}~9~15)}$'' prints all
triples of integers up to $9$ that sum to $15$.

The \textsf{triple} example is mostly a toy, but these primitives can be practically useful. In
the paper Danvy and Filinski show an evaluator for {\em nondeterministic finite automata} (NFAs),
which is straightforward to implement thanks to operators like \textsf{flip} and \textsf{fail}.
The implementation is not especially instructive, so I will not repeat it here.

\subsection{Generalizing to More Contexts}

The final contribution in {\em Abstracting Control} is an idea for a {\em family} of shift and
reset operators that operate on an indexed family of program contexts. They introduce operators
\begin{center}
  $\textsf{\color{ACMDarkBlue}shift}_i~k~\textsf{\color{ACMDarkBlue}in}~e$ \hspace{5mm}
  and \hspace{5mm} $\reset{e}_i$
\end{center}
which access and delimit the $i$th enclosing context respectively. The goal here is to be able to
associate different simulated effects (e.g. error handling and nondeterminism) with different
indexed operators; without such a mechanism the effects do not compose well.

These operators can also be given meaning via the ECPS translation, but with a bit more hassle. A
program that uses indices up to $n$ will need to be translated $n + 1$ times. The translations
are defined by mutual induction on term structure (as before) and on operator indices.
\begin{align*}
  \cps{\textsf{\color{ACMDarkBlue}shift}_0~k~\textsf{\color{ACMDarkBlue}in}~e} &= e[k \mapsto \textsf{id}] \\
  \cps{\textsf{\color{ACMDarkBlue}shift}_{n + 1}~k~\textsf{\color{ACMDarkBlue}in}~e} &= 
    \lambdaE{\kappa}{\textsf{\color{ACMDarkBlue}shift}_{n}~k'~\textsf{\color{ACMDarkBlue}in}~(\cps{e}~\textsf{id})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~\reset{k'~(\kappa~x)}_n}}]} \\
  \cps{\reset{e}_0} &= e \\
  \cps{\reset{e}_{n + 1}} &= \lambdaE{\kappa}{\kappa~\reset{\cps{e}~\textsf{id}}_n}
\end{align*}

\outline{Something here?}

\section{{\em Comprehending Monads}} \label{sec:wadler}

Monads were originally introduced to the programming languages literature by
\citet{moggi1991notions}, but {\em Comprehending Monads} by \citet{wadler1990comprehending}
helped to popularize them. A monad is a mathematical structure commonly used in category theory.
There, a monad is defined as a \underline{functor} with certain associated
\underline{\smash{operations}} that obey a set of \underline{laws}.

For our purposes, a {\em functor} is a type constructor (e.g. \textsf{List}, \textsf{Maybe}) with
an operation \textsf{map} that applies a function ``under'' the type constructor. For example,
\[ \map{List}{f}{\overline{x}} \]
applies $f$ to all elements of $\overline{x}$ and 
\[ \map{Maybe}{f}{\overline{x}} \]
applies $f$ to the value contained in $\overline{x}$, if one exists. (I adopt Wadler's convention
of writing monadic values---values wrapped in the monad's type constructor---with a bar over the
variable name.) Map must obey two laws:
\begin{align*}
  \textsf{map}~\textsf{id} &= \textsf{id} \\
  \textsf{map}~(g \circ f) &= \textsf{map}~g \circ \textsf{map}~f
\end{align*}
The first law says that mapping the identity function over a structure does nothing---this is
necessary to make sure that \textsf{map} does not change the structure itself, only the elements
in the structure. The second law says that \textsf{map} is well-behaved with respect to function
composition.

In order for a functor to be a monad, it needs two more operations: \textsf{unit} and \textsf{join}.
Applying \textsf{unit} to a value injects that value into the monad. In the case of \textsf{List},
\[ \unit{List}{x} = \textsf{singleton}~x = [x], \]
which is the simplest way to inject a value into a list. Likewise,
\[ \unit{Maybe}{x} = \textsf{Just}~x. \]
There are relatively few functors with no \textsf{unit}, but some do exist. For example, 
$(\textsf{\color{ACMDarkBlue} int},\, -)$ is a functor, with
\[ \textsf{map}^{(\textsf{\color{ACMDarkBlue} int},\, -)}~f~(n,\, x) = (n,\, f~x), \]
but there is no way to know which integer to choose when trying to define \textsf{unit}.

The \textsf{join} operation works on ``doubled up'' instances of the monad. It takes a doubled
value like
\begin{center}
  ``$[[1, 2], [3, 4]]$''\hspace{5mm}or\hspace{5mm}``$\textsf{Just}~(\textsf{Just}~5)$''
\end{center}
and flattens it down to a single application of the monad like
\begin{center}
  ``$[1, 2, 3, 4]$''\hspace{5mm}or\hspace{5mm}``$\textsf{Just}~5$''.
\end{center}
To be precise,
\begin{center}
  \begin{tabular}{lll}
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
\end{center}
Functors with a \textsf{join} operation admit a special kind of function chaining called {\em
Kleisli composition}. Given a functor $F$ and two functions $f: \beta \to F~\gamma$ and $g: \alpha
\to F~\beta$, we can write
\[ f \circ_F g = \textsf{join}^F \circ map^F~f \circ g \]
that composes them together while properly keeping track of $F$. This is useful in programming
because it means that monadic operations like $f$ and $g$ can be built up individually and
composed together to build larger programs. A very similar construction can also yield the
``\textsf{bind}'' (or ``\textsf{>>=}'') definition of monads that is ubiquitous in modern
programming languages. Though these alternative views of monads are probably more useful for
programming, they are slightly harder to work with for theory so I will continue to Wadler's
notation.

As mentioned, these operations must obey certain laws in order for the structure to truly be a
monad:
\begin{align}
  \textsf{map}~f \circ \textsf{unit} &= \textsf{unit} \circ f \\
  \textsf{map}~f \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~(\textsf{map}~f) \\
  \textsf{join} \circ \textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{map}~\textsf{unit} &= \textsf{id} \\
  \textsf{join} \circ \textsf{join} &= \textsf{join} \circ \textsf{map}~\textsf{join}
\end{align}
Intuitively, laws (1) and (2) show that \textsf{unit} and \textsf{join} are ``well-behaved'' with
respect to \textsf{map},\footnote{Wadler notes that (1) and (2) hold automatically if the
operations $\textsf{map}$, $\textsf{unit}$, and $\textsf{join}$ are implemented as {\em
polymorphic functions}, but those details are out of scope for this
report~\cite{wadler1989theorems}.} and laws (3-5) require that \textsf{unit} and \textsf{join}
are the {\em simplest} ways to do their respective tasks. For example, suppose we tried to implement
\[ \unit{List}{x} = [x,\, x,\, x,\, x,\, x]. \]
Technically this does inject a value into a list, but it is not the simplest way to do so and
\[
  (\textsf{join}^{\textsf{List}} \circ \textsf{unit}^{\textsf{List}})~[3] = [3,\, 3,\, 3,\, 3,\, 3] \neq \textsf{id}~[3]. \\
\]
Likewise, the laws require that \textsf{join} flattens a stack of monads in a simple, predictable
way.

All of this background on monads and their operations is important, but in practice using these
operations directly does not make for particularly elegant code. In the next section, I explore
Wadler's use of {\em comprehensions} to make programming with monads much more intuitive.

\subsection{Comprehensions}

List comprehensions are based on set-builder notation (e.g. $\{n + 1 \mid n \in \mathbb{N}\}$)
and were first introduced to programming by the SETL language in the
1960's~\cite{schwartz2012programming}. Since then, list comprehensions have become ubiquitous,
appearing in many modern languages including Racket, Python, and even C++ (with some abuse of
operator overloading). As shown in Section \ref{sec:introduction},
\[ [(x, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{List}} \]
represents the Cartesian product of $\overline{x}$ and $\overline{y}$ in list-comprehension notation
\[ \{(x, y) \mid x \in X \wedge y \in Y\} \]
represents the Cartesian product of $X$ and $Y$ in standard mathematical notation.

Wadler starts off by adding a formal syntax for list comprehensions to a standard lambda calculus:
\begin{align*}
 e &\Coloneqq \cdots \mid [e \mid q] \\
 q &\Coloneqq \Lambda \mid x \leftarrow e \mid (q_1;\ q_2)
\end{align*}
The symbol $q$ stands for {\em qualifiers}---qualifiers can be empty (denoted $\Lambda$), a {\em
generator} for a variable $x$, or a composition of two qualifiers. This list comprehension syntax
is defined by the following rules:
\begin{align*}
  [e \mid \Lambda] &= \textsf{singleton}~e \\
  [e_1 \mid x \leftarrow e_2] &= \textsf{map}~(\lambdaE{x}{e_1})~e_2 \\
  [e \mid (q_1;\ q_2)] &= \textsf{flatten}~[[e \mid q_2] \mid q_1] \\
\end{align*}
Wadler noticed that lists are not the only structure for which these operations make
sense---recall that \textsf{singleton} is $\textsf{unit}^{\textsf{List}}$ and \textsf{flatten} is
$\textsf{join}^{\textsf{List}}$. This means that comprehensions actually work for any monad, with
their behavior defined by:
\begin{align*}
  [e \mid \Lambda]^{\textsf{M}} &= \unit{M}{e} \\
  [e_1 \mid x \leftarrow e_2]^{\textsf{M}} &= \map{M}{(\lambdaE{x}{e_1})}{e_2} \\
  [e \mid (q_1;\ q_2)]^{\textsf{M}} &= \join{M}{[[e \mid q_2]^{\textsf{M}} \mid q_1]^{\textsf{M}}} \\
\end{align*}
Rewriting the \textsf{Maybe} example from Section \ref{sec:introduction} using these rules (and
rewriting a bit) results in
\begin{align*}
  &[ y \mid x \leftarrow \textsf{thisMightFail}~() ;\ y \leftarrow \textsf{thisMightFailToo}~{x} ]^{\textsf{Maybe}} = \\
  &\join{}{(\map{}{\textsf{thisMightFailToo}}{(\textsf{thisMightFail}~())})}
\end{align*}
which has the semantics we expect: if either computation fails the final result is \nothing\,
otherwise the result is a single \textsf{Just} wrapped around the final result.

\subsection{Examples of Monads} \label{sec:monad-examples}

In this section I list of the most common and useful monads that Wadler points out.

\subsubsection{List and Maybe}
The first two monads have been running examples in this section, so I will not spend much time on
them here. Here is the full definition of the list monad
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type List}$~$\alpha$ & $=$ & $\textsf{Nil} \mid \textsf{Cons}~\alpha~(\textsf{List}~\alpha)$ \\
    $\map{List}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nil} \to \textsf{Nil};\ \textsf{Cons}~y~\overline{y} \to \textsf{Cons}~(f~y)~(\map{List}{f}{\overline{y}})}$ \\
    $\unit{List}{x}$ & $=$ & $\textsf{singleton}~x$ \\
    $\join{List}{\doverline{x}}$ & $=$ & $\textsf{flatten}~\doverline{x}$
  \end{tabular}
\end{center}
and the maybe monad
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Maybe}$~$\alpha$ & $=$ & $\textsf{Nothing} \mid \textsf{Just}~\alpha$ \\
    $\map{Maybe}{f}{\overline{x}}$ & $=$ & $\caseOf{\overline{x}}{\textsf{Nothing} \to \textsf{Nothing};\ \textsf{Just}~y \to \textsf{Just}~(f~y)}$ \\
    $\unit{Maybe}{x}$ & $=$ & $\textsf{Just}~x$ \\
    $\join{Maybe}{\doverline{x}}$ & $=$ & $\caseOf{\doverline{x}}{\textsf{Just}~(\textsf{Just}~x) \to \textsf{Just}~x;\ \_ \to \textsf{Nothing}}$
  \end{tabular}
\end{center}
The type definitions are informal and based loosely on Haskell's algebraic data type syntax.

\subsubsection{Identity and Strictness}
The simplest possible monad is the identity functor.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Id}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Id}{f}{\overline{x}}$ & $=$ & $f~\overline{x}$ \\
    $\unit{Id}{x}$ & $=$ & $x$ \\
    $\join{Id}{\doverline{x}}$ & $=$ & $\doverline{x}$
  \end{tabular}
\end{center}
This monad is almost not worth mentioning, but it does have one cute use-case: comprehension
syntax for the identity monad subsumes \textsf{\color{ACMDarkBlue} let} binding. Rather than
write
\begin{center}
  $\letIn{x}{e_1}{e_2}$ \hspace{5mm} we can write \hspace{5mm} $[e_2 \mid x \leftarrow e_1]^{\textsf{Id}}$.
\end{center}
There is no real reason to prefer this syntax, so I will continue to use
$\textsf{\color{ACMDarkBlue} let}$, but it is a fun observation.

A slight variation on the identity monad, the {\em strictness} monad, does actually have some
interesting uses.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Str}$~$\alpha$ & $=$ & $\alpha$ \\
    $\map{Str}{f}{\overline{x}}$ & $=$ & $\ifThenElse{\overline{x} \neq \bot}{f~\overline{x}}{\bot}$ \\
    $\unit{Str}{x}$ & $=$ & $x$ \\
    $\join{Str}{\doverline{x}}$ & $=$ & $\doverline{x}$
  \end{tabular}
\end{center}
The only difference here is the implementation of $\textsf{map}$---the strictness monad enforces
call-by-value application of the function $f$. In a call-by-name language with divergence, the
strictness monad provides a convenient syntax for controlling evaluation order.

\subsubsection{State} \label{sec:wadler:state}
Another monad we have already seen is the \textsf{State} monad. The state monad makes it possible
to write code that {\em looks} stateful, even though under the hood there is a state parameter
being passed around. Assuming some fixed state type, $\sigma$, the state monad is defined as:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type State}$~$\alpha$ & $=$ & $\sigma \to (\alpha,\, \sigma)$ \\
    $\map{State}{f}{\overline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(x,\, s')}{\overline{x}~s}{(f~x,\, s')}}$ \\
    $\unit{State}{x}$ & $=$ & $\lambdaE{s}{(x,\, s)}$ \\
    $\join{State}{\doverline{x}}$ & $=$ & $\lambdaE{s}{\letIn{(\overline{x},\, s')}{\doverline{x}~s}{\overline{x}~s'}}$
  \end{tabular}
\end{center}
As with many monads, the real magic comes with some auxiliary operations. In this case, defining
\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\lambdaE{s}{(s,\, s)}$ \\
    $\textsf{put}~s'$ & $=$ & $\lambdaE{s}{(x,\, s')}$
  \end{tabular}
\end{center}
gives us everything we need to implement the example from the introduction:
\[ [k \mid i \leftarrow \textsf{get};\ \_ \leftarrow \textsf{put}~(i + 3);\ j \leftarrow \textsf{get};\ \_ \leftarrow \textsf{put}~(j * 7);\ k \leftarrow \textsf{get}]^{\textsf{State}} \]
This is a good time to mention that modern languages with monads use a variation of Wadler's
comprehension syntax called ``do-notation.'' The \textsf{do} from the Haskell example in Section
\ref{sec:introduction},
\[ \textsf{\color{ACMDarkBlue} do}\ \{\ i \leftarrow \textsf{get};\ \textsf{put}~(i + 3);\ j \leftarrow \textsf{get};\ \textsf{put}~(j * 7);\ \textsf{get}\ \}, \]
has two ergonomic optimizations. First, the whole expression evaluates to the final qualifier in
the list, which often saves a few characters, and second bare expressions are allowed as
qualifiers. These facts together mean that do-notation is a bit easier to use than Wadler's
syntax, even if it is morally quite similar. I discuss a bit more about the state of the art
in Section \ref{sec:conclusion}.

\subsubsection{Reader}
(Wadler calls this {\em State Reader}, but modern languages almost exclusively drop the word
``state'' and just call this monad ``reader.'') The \textsf{Reader} monad is a simplification of
the \textsf{State} monad, and arose when Wadler realized some parts of a \textsf{State}-monad
computation do not actually change the state (and in fact they {\em should} not).
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Reader}$~$\alpha$ & $=$ & $\rho \to \alpha$ \\
    $\map{Reader}{f}{\overline{x}}$ & $=$ & $\lambdaE{r}{f~(\overline{x}~r)}$ \\
    $\unit{Reader}{x}$ & $=$ & $\lambdaE{r}{x}$ \\
    $\join{Reader}{\doverline{x}}$ & $=$ & $\lambdaE{r}{(\doverline{x}~r)~r}$
  \end{tabular}
\end{center}
Despite its relative simplicity, the \textsf{Reader} monad is quite powerful. After defining the
auxiliary operation,
\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\lambdaE{r}{r}$,
  \end{tabular}
\end{center}
\textsf{Reader} can be used to keep track of configuration values, environment information, and
any other static value that would otherwise be cumbersome to pass around. Readers are also safer
and more flexible than global variables or constants, since they can be set midway through the
computation and are guaranteed to not change later on.

\subsubsection{Nondeterminism (Set)}
The nondeterminism monad (also known as the set monad) enables direct-style programming of
nondeterministic algorithms.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type ND}$~$\alpha$ & $=$ & $2^\alpha$ \\
    $\map{ND}{f}{\overline{x}}$ & $=$ & $\{f~x \mid x \in \overline{x}\}$ \\
    $\unit{ND}{x}$ & $=$ & $\{x\}$ \\
    $\join{ND}{\doverline{x}}$ & $=$ & $\bigcup \doverline{x}$
  \end{tabular}
\end{center}
A value in \textsf{ND} is a set of possible outcomes. Together with an auxiliary operator for
nondeterministic choice
\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\varnothing$ \\
    $\textsf{flip}$ & $=$ & $\{\textsf{\color{ACMDarkBlue} true},\, \textsf{\color{ACMDarkBlue} false}\}$,
  \end{tabular}
\end{center}
this monad can be extremely useful.

\subsubsection{Parser}
Finally, here is a more complicated monad: \textsf{Parser}. Parsers can be seen as a combination
of two monads: \textsf{List} and \textsf{State}.\footnote{Technically \textsf{Parser} can be
formed using a {\em monad transformer}, but those were developed later on~\cite{liang1995monad}.}
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Parser}$~$\alpha$ & $=$ & $\textsf{String} \to \textsf{List}~(\alpha,\, \textsf{String})$ \\
    $\map{Parser}{f}{\overline{x}}$ & $=$ & $\lambdaE{i}{[(f~x,\, i') \mid (x,\, i') \leftarrow \overline{x}~i]^{\textsf{List}}}$ \\
    $\unit{Parser}{x}$ & $=$ & $\lambdaE{i}{[(x, i)]^{\textsf{List}}}$ \\
    $\join{Parser}{\doverline{x}}$ & $=$ & $\lambdaE{i}{[(x,\, i'') \mid (\overline{x},\, i') \leftarrow \doverline{x}~i;\ (x,\ i'') \leftarrow \overline{x}~i']^{\textsf{List}}}$
  \end{tabular}
\end{center}
Intuitively, the \textsf{String} argument is the parser input, and the result is a list of
potential parses. Each potential parse is made up of a result of type $\alpha$ and the
\textsf{String} that remains after parsing. There are a number of auxiliary operations that are
useful for \textsf{Parser} (often called {\em parser combinators}), including ``\textsf{satisfy}''
which only parses a character that matches a given predicate and ``\textsf{many}'' which applies a
parser multiple times. An amazing amount of research has been done on the \textsf{Parser} monad
and parser combinators~\cite{hutton1996monadic, leijen2001parsec}, and it is still an active
area~\cite{willis2020staged}.

\subsection{Translation}

Monads can be used to replicate a variety of effects in a pure way; it would be nice if there was
a way to translate an impure program into the equivalent monadic program. Wadler gives two such
translations, one for call-by-value and another for call-by-name. The result of both translations
is a pure program in the non-strict lambda calculus with comprehensions that we have been using for
examples.

Suppose our source language is our lambda running calculus with products and projections. The
call-by-value translation ``$e^*$'' lifts a program of type
\begin{center}
$\alpha \to \beta$ \hspace{5mm} to one of type \hspace{5mm} $\alpha \to \textsf{M}~\beta$
\end{center}
for some monad \textsf{M}. Intuitively, the translated function takes a value of type $\alpha$ and
returns a {\em computation} of type $\beta$. The monad captures the effects of that computation.
Here is the translation in full:
\begin{align*}
  x^* &= [x]^\textsf{M} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{M} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{M} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{M}
\end{align*}
Each of these rules has a straightforward computational meaning. For example
\[ (e_1~e_2)^* = [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{M} \]
says that in order to evaluate a (potentially effectful) application, evaluate $e_1$ to $f$, then
evaluate $e_2$ to $x$, and then evaluate the application ``$f~x$'', all the while keeping track
of any side-effects that are produced.

If our source language was actually effectful, for example with built-in effects
``$\textsf{\color{ACMDarkBlue}get}$'' and ``$\textsf{\color{ACMDarkBlue}put}$'', we could choose
$\textsf{M} = \textsf{State}$ and add translations
\begin{align*}
  \textsf{\color{ACMDarkBlue}get}^* &= [x \mid x \leftarrow \textsf{get}]^\textsf{State} \\
  (\textsf{\color{ACMDarkBlue}put}~e)^* &= [u \mid x \leftarrow e^*;\ u \leftarrow \textsf{put}~x]^\textsf{State}
\end{align*}
where \textsf{get} and \textsf{put} are defined as in Section \ref{sec:monad-examples}. If we
were to prove once and for all that the monadic version has the same semantics as the impure
version, further analysis could be done on the pure language instead. In addition, this kind of
translation might streamline the implementation of a compiler or interpreter.

Wadler also gives a call-by-name version of the translation, which lifts a program of type
\begin{center}
$\alpha \to \beta$ \hspace{5mm} to one of type \hspace{5mm} $\textsf{M}~\alpha \to \textsf{M}~\beta$.
\end{center}
In call-by-name, the function arguments are computations as well. The translation is written
$e^\dagger$:
\begin{align*}
  x^\dagger &= x \\
  (\lambdaE{x}{e})^\dagger &= [\lambdaE{x}{e^\dagger}]^\textsf{M} \\
  (e_1~e_2)^\dagger &= [y \mid f \leftarrow e_1^\dagger;\ y \leftarrow f~e_2^\dagger]^\textsf{M} \\
  (e_1,\, e_2)^\dagger &= [(e_1^\dagger,\, e_2^\dagger)]^\textsf{M} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^\dagger &= [y \mid x \leftarrow e^\dagger;\ y \leftarrow \textsf{\color{ACMDarkBlue}fst}~x]^\textsf{M}
\end{align*}
If the source language were call-by-name, this translation might be similarly useful to the first.

\section{Common Design Patterns} \label{sec:patterns}
The zoo of monads presented by Wadler are a nice basis by which to evaluate other compositional
abstractions. As one might hope, delimited continuations are able to represent many of the monads
that Wadler mentions---a blog post by \citet{xia_2019} from 2019 describes these constructions
incredibly well, and many of the constructions here are inspired by that presentation.

Reimplementing a monadic design pattern using delimited continuations requires defining (1) the
primitive operations that are core to the abstraction and (2) a \textsf{run} function that
delimits the scope of the computation.

\subsubsection{Maybe}
When we saw the \textsf{Maybe} monad earlier we did not talk about any primitive operations, but
there is one obvious one: \textsf{abort}. Abort throws the simplest kind of exception, throwing
away the computation and returning \textsf{Nothing}.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{abort}$ & $=$ & $\shift{k}{\textsf{Nothing}}$ \\
    $\textsf{run}^{\textsf{Maybe}}~c$ & $=$ & $\reset{\textsf{Just}~c}$
  \end{tabular}
\end{center}
The definition of \textsf{abort} here reflects that intuition. The \textsf{run} function simply
wraps the computation result in \textsf{Just}, unless there is an \textsf{abort}.

\subsubsection{Identity}
Identity can be encoded without using continuations at all, of course, but this presentation is
more instructive.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{noop}~x$ & $=$ & $\shift{k}{k~x}$ \\
    $\textsf{run}^{\textsf{Id}}~c$ & $=$ & $\reset{c}$
  \end{tabular}
\end{center}
Here the \textsf{noop} operation shows how to do nothing using delimited continuations---just
call the continuation. All \textsf{run} needs to do is delimit the continuation scope.

\subsubsection{State}
This example is considerably more involved. As with the state monad, encoding state with
continuations is about capturing state in function parameters and return values.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{get}$ & $=$ & $\shift{k}{\lambdaE{s}{k~s~s}}$ \\
    $\textsf{put}~s'$ & $=$ & $\shift{k}{\lambdaE{s}{k~()~s'}}$ \\
    $\textsf{run}^{\textsf{State}}~c~i$ & $=$ & $\reset{(\lambdaE{v}{\lambdaE{s}{v}})~c}~i$
  \end{tabular}
\end{center}
Our continuations take two parameters: the first is the computation value, and the second is the
new state. The \textsf{get} operation takes in the current state and continues with that state as
both the computation value and the new state. The \textsf{put} operation ignores the current
state and continues with unit as the computation value and $s'$ as the new state. The
\textsf{run} function takes an initial state and passes it in {\em outside} of the continuation
scope.

As with the monadic version, these operations can be used to simulate stateful code. For example,
this program
\begin{align*}
\textsf{run}^{\textsf{State}}~(& \textsf{\color{ACMDarkBlue}let}~i = \textsf{get}~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{put}~(i + 3); \\
& \textsf{\color{ACMDarkBlue}let}~j = \textsf{get}~\textsf{\color{ACMDarkBlue}in} \\
& \textsf{put}~(j * 7); \\
& \textsf{get})~3
\end{align*}
returns 42 as one might expect, and does so without references or other build-in state constructs.

\subsubsection{Reader}
\textsf{Reader} is strictly simpler than \textsf{State}, but we can use the same definitions anyway.
\begin{center}
  \begin{tabular}{lll}
    $\textsf{ask}$ & $=$ & $\textsf{get}$ \\
    $\textsf{run}^{\textsf{Reader}}$ & $=$ & $\textsf{run}^{\textsf{State}}$
  \end{tabular}
\end{center}
We just rename \textsf{get} to \textsf{ask} and don't provide a \textsf{put} operation.

\subsubsection{Nondeterminism}
We already saw one way of encoding nondeterminism using continuations (see Section
\ref{sec:danvy:nondet}), but here is another one that is a bit closer to the monadic approach. We
again make use of sets to define the primitive operations $\textsf{fail}$ and $\textsf{flip}$:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{fail}$ & $=$ & $\shift{k}{k~\varnothing}$ \\
    $\textsf{flip}$ & $=$ & $\shift{k}{k~\textsf{\color{ACMDarkBlue} true} \cup k~\textsf{\color{ACMDarkBlue} false}}$ \\
    $\textsf{run}^{\textsf{ND}}~c$ & $=$ & $\reset{(\lambdaE{v}{\{v\}})~c}$
  \end{tabular}
\end{center}
Our implementation of \textsf{run} lifts computations into singleton sets, and our operators call
the continuation on whatever values should be considered---either nothing in the case of
\textsf{fail} or both $\textsf{\color{ACMDarkBlue} true}$ and $\textsf{\color{ACMDarkBlue}
false}$ in the case of $\textsf{flip}$.

\subsection{Are there more?}
This is just the tip of the iceberg when it comes to monadic design patterns that can be
implemented using delimited continuations---in fact a few years after {\em Abstracting Control},
\citet{filinski1994representing} published another paper showing that delimited continuations can
represent all ``pure'' monads (i.e. those whose \textsf{map}, \textsf{join}, and \textsf{bind}
operations can be implemented in a pure language). We discuss this result further in Section
\ref{sec:conclusion}.

\section{The Continuation Monad} \label{sec:contmonad}
So far we have seen a number of examples of monads, but \citeauthor{wadler1990comprehending}
actually discusses one more: the {\em continuation monad}. Given a result type, $\rho$, the
continuation monad has operations:
\begin{center}
  \begin{tabular}{lll}
    $\textsf{type Cont}$~$\alpha$ & $=$ & $(\alpha \to \rho) \to \rho$ \\
    $\map{Cont}{f}{\overline{x}}$ & $=$ & $\lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\kappa~(f~x)})}$ \\
    $\unit{Cont}{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~x}$ \\
    $\join{Cont}{\doverline{x}}$ & $=$ &
      $\lambdaE{\kappa}{\doverline{x}~(\lambdaE{\overline{x}}{\overline{x}~\kappa})}$
  \end{tabular}
\end{center}
Programs written using the continuation monad are automatically in a continuation-passing style:
sequencing
\[ [(x, y) \mid x \leftarrow \overline{x};\ y \leftarrow \overline{y}]^{\textsf{Cont}} \]
expands to
\[ \lambdaE{\kappa}{\overline{x}~(\lambdaE{x}{\overline{y}~(\lambdaE{y}{\kappa~(x,\, y)})})}, \]
which is precisely the way a tuple of expressions would be evaluated in CPS.

You might see where this is going: we can implement the delimited continuation operators, shift and
reset, as operations in the continuation monad! We can define
\begin{center}
  \begin{tabular}{lll}
    $\textsf{shift}$~$f$ & $=$ & $\lambdaE{\kappa}{f~(\lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}})~(\lambdaE{x}{x})}$ \\
    $\textsf{reset}$~$\overline{x}$ & $=$ & $\lambdaE{\kappa}{\kappa~(\overline{x}~(\lambdaE{x}{x}))}$
  \end{tabular}
\end{center}
and then we can implement examples from \citeauthor{danvy1989functional} like
\[
  [x + y \mid x \leftarrow [1];\ y \leftarrow \textsf{reset}~[u + v \mid u \leftarrow [10];\ v \leftarrow \textsf{shift}(\lambdaE{k}{[b \mid a \leftarrow k~100;\ b \leftarrow k~a]})]]. \\
\]
Admittedly, this is not as terse as
\[ 1 + \reset{10 + \shift{k}{k~(k~100)}}, \]
but it says and does the same thing, and critically neither version relies on manually managing
continuation parameters.

We can tie everything together by inspecting the results of Wadler's call-by-value translation into
the continuation monad.
\begin{align*}
  x^* &= [x]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~x} \\
  (\lambdaE{x}{e})^* &= [\lambdaE{x}{e^*}]^\textsf{Cont} &= \lambdaE{\kappa}{\kappa~(\lambdaE{x}{e^*})} \\
  (e_1~e_2)^* &= [y \mid f \leftarrow e_1^*;\ x \leftarrow e_2^*;\ y \leftarrow f~x]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{f}{e_2^*~(\lambdaE{x}{f~x~\kappa})})} \\
  (e_1,\, e_2)^* &= [(x, y) \mid x \leftarrow e_1^*;\ y \leftarrow e_2^*]^\textsf{Cont} &= \lambdaE{\kappa}{e_1^*~(\lambdaE{x}{e_2^*~(\lambdaE{y}{\kappa (x,\, y)})})} \\
  (\textsf{\color{ACMDarkBlue}fst}~e)^* &= [\textsf{\color{ACMDarkBlue}fst}~x \mid x \leftarrow e^*]^\textsf{Cont} &= \lambdaE{\kappa}{e^*~(\lambdaE{x}{\textsf{\color{ACMDarkBlue} fst}~x)}}
\end{align*}
This translation is exactly the first CPS translation from Section \ref{sec:danvy}! And if we
extend the transformation to include our embeddings of shift and reset into the continuation
monad,
\begin{align*}
  (\shift{k}{e})^* &= \textsf{shift}~(\lambdaE{k}{e^*}) &= (\lambdaE{\kappa}{e^*~\textsf{id}})[k \mapsto \lambdaE{x}{\lambdaE{\kappa'}{\kappa'~(\kappa~x)}}] \\
  \reset{e}^* &= \textsf{reset}~e^* &= \lambdaE{\kappa}{\kappa~(e^*~\textsf{id})},
\end{align*}
we arrive at the ECPS translation.

Wadler did point out the former observation---in fact, he also observed that the call-by-name
translation results in the unusual call-by-name version of the CPS translation---but I was very
pleased to complete the picture and add translations for shift and reset. Of course, the shift
and reset translations mean that this extension of $(\cdot)^*$ is no longer generic over the
choice of monad, but the same goes for the translations of any monad-specific operations (e.g.
get and put).

\section{Conclusion} \label{sec:conclusion}

The connection between monads and continuations has already been explored.
\citet{filinski1994representing} provides a detailed account of how monadic computations can be
represented using delimited continuations. While my constructions in Section \ref{sec:patterns}
are ad-hoc and dependent on the particular monad in question, Filinski managed to give a
translation that represents monadic patterns with continuations once-and-for-all. Still, the
ad-hoc connections are valuable: they provide more granular intuition than Filinski's
presentation, and they are all entirely pure, while Filinski's formulation relies on a reference
cell.\footnote{This is a nit-picky point, but requiring an impure reference cell is a compromise.}

\outline{
  \begin{itemize}
    \item Some related work
    \item Mention algebraic effects
  \end{itemize}
}

\begin{acks}
\end{acks}

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}

% \begin{quote}
%   [In some languages,] it may be possible to jump out of an expression and then later jump back
%   into it again and resume the process of evaluation. Continuations are sufficiently powerful to
%   deal with such a situation. (This could not be taken to imply approval of jumps back into
%   expressions as a language design feature—but if a language can specify something, however odd,
%   the method used to give its formal semantics must be powerful enough to describe it.)
%   \cite{strachey2000continuations}
% \end{quote}
